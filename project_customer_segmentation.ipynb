PROBLEM

In this project, I will delve deeper into the online retail sector, one of the rapidly developing industries, by analyzing a transactional dataset from a retailer with a branch in the UK, provided by the UCI Machine Learning Repository. This dataset records all transactions from 2010 to 2011.

My main goal is to increase the effectiveness of marketing strategies and boost sales through customer segmentation. I will transform the transactional data into a customer-focused dataset by creating new features to assist in grouping customers into distinct segments using the K-means clustering algorithm.

This segmentation will help me understand the characteristics and behaviors of each customer group. Based on these insights, I will develop a recommendation system that suggests best-selling products to customers in each segment who have not previously purchased those items, thereby enhancing marketing efficiency and driving sales growth.
*** Ti·∫øng Vi·ªát ***
Trong d·ª± √°n n√†y, t√¥i s·∫Ω ƒëi s√¢u ph√¢n t√≠ch lƒ©nh v·ª±c b√°n l·∫ª tr·ª±c tuy·∫øn ‚Äì m·ªôt trong nh·ªØng ng√†nh ƒëang ph√°t tri·ªÉn nhanh ch√≥ng ‚Äì th√¥ng qua b·ªô d·ªØ li·ªáu giao d·ªãch t·ª´ m·ªôt nh√† b√°n l·∫ª c√≥ chi nh√°nh t·∫°i V∆∞∆°ng qu·ªëc Anh, ƒë∆∞·ª£c cung c·∫•p b·ªüi UCI Machine Learning Repository. B·ªô d·ªØ li·ªáu n√†y ghi l·∫°i to√†n b·ªô c√°c giao d·ªãch t·ª´ nƒÉm 2010 ƒë·∫øn 2011.

M·ª•c ti√™u ch√≠nh c·ªßa t√¥i l√† n√¢ng cao hi·ªáu qu·∫£ chi·∫øn l∆∞·ª£c ti·∫øp th·ªã v√† th√∫c ƒë·∫©y doanh s·ªë th√¥ng qua ph√¢n kh√∫c kh√°ch h√†ng. T√¥i s·∫Ω chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu giao d·ªãch th√†nh d·ªØ li·ªáu t·∫≠p trung v√†o kh√°ch h√†ng b·∫±ng c√°ch t·∫°o ra c√°c ƒë·∫∑c tr∆∞ng m·ªõi, t·ª´ ƒë√≥ h·ªó tr·ª£ vi·ªác ph√¢n nh√≥m kh√°ch h√†ng th√†nh c√°c ph√¢n kh√∫c ri√™ng bi·ªát b·∫±ng thu·∫≠t to√°n ph√¢n c·ª•m K-means.

Vi·ªác ph√¢n kh√∫c n√†y s·∫Ω gi√∫p t√¥i hi·ªÉu r√µ ƒë·∫∑c ƒëi·ªÉm v√† h√†nh vi c·ªßa t·ª´ng nh√≥m kh√°ch h√†ng. D·ª±a tr√™n nh·ªØng hi·ªÉu bi·∫øt n√†y, t√¥i s·∫Ω ph√°t tri·ªÉn m·ªôt h·ªá th·ªëng g·ª£i √Ω, ƒë·ªÅ xu·∫•t c√°c s·∫£n ph·∫©m b√°n ch·∫°y nh·∫•t cho nh·ªØng kh√°ch h√†ng trong t·ª´ng ph√¢n kh√∫c ‚Äì nh·ªØng ng∆∞·ªùi ch∆∞a t·ª´ng mua c√°c s·∫£n ph·∫©m ƒë√≥ ‚Äì t·ª´ ƒë√≥ tƒÉng c∆∞·ªùng hi·ªáu qu·∫£ ti·∫øp th·ªã v√† th√∫c ƒë·∫©y tƒÉng tr∆∞·ªüng doanh s·ªë.
OBJECTIVES

-Data Cleaning & Transformation:
 Clean the dataset by handling missing values,duplicates, and outliers, preparing it for effective clustering.
-Feature Engineering:
 Develop new features based on the transactional data to create a customer-centric dataset, setting the foundation for customer segmentation.
-Data Preprocessing:
 Undertake feature scaling and dimensionality reduction to streamline the data, enhancing the efficiency of the clustering process.
-Customer Segmentation using K-Means Clustering:
 Segment customers into distinct groups using K-means, facilitating targeted marketing and personalized strategies.
Step 1 | Setup and Initialization
import kagglehub

# Download latest version
path = kagglehub.dataset_download("carrie1/ecommerce-data")

print("Path to dataset files:", path)
# Ignore warnings
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import plotly.graph_objects as go
from matplotlib.colors import LinearSegmentedColormap
from matplotlib import colors as mcolors
from scipy.stats import linregress
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.cluster import KMeans
from tabulate import tabulate
from collections import Counter

%matplotlib inline
# Initialize Plotly for use in the notebook
from plotly.offline import init_notebook_mode
init_notebook_mode(connected=True)
# Configure Seaborn plot styles: Set background color and use white
sns.set(rc={'axes.facecolor': '#fcf0dc'}, style='white')
Step 1.2 | Loading the Dataset
df = pd.read_csv(r"C:\Users\AORUS 15\Desktop\Project\Final Project -CoderSchool\data.csv", encoding="ISO-8859-1")

## M√¥ t·∫£ d·ªØ li·ªáu (Dataset Description)

| Bi·∫øn         | √ù nghƒ©a chi ti·∫øt                                                                                   |
|--------------|----------------------------------------------------------------------------------------------------|
| InvoiceNo    | M√£ h√≥a ƒë∆°n cho m·ªói giao d·ªãch. N·∫øu b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ C (v√≠ d·ª•: C123456), ƒë√≥ l√† h√≥a ƒë∆°n h·ªßy           |
| StockCode    | M√£ s·∫£n ph·∫©m ‚Äì ƒë·ªãnh danh duy nh·∫•t cho t·ª´ng lo·∫°i s·∫£n ph·∫©m                                            |
| Description  | M√¥ t·∫£ t√™n s·∫£n ph·∫©m ‚Äì c√≥ th·ªÉ ch·ª©a l·ªói ch√≠nh t·∫£ ho·∫∑c b·ªã thi·∫øu n·∫øu d·ªØ li·ªáu kh√¥ng s·∫°ch                  |
| Quantity     | S·ªë l∆∞·ª£ng s·∫£n ph·∫©m ƒë∆∞·ª£c mua trong giao d·ªãch. C√≥ th·ªÉ √¢m n·∫øu l√† ho√†n tr·∫£                              |
| InvoiceDate  | Ng√†y gi·ªù th·ª±c hi·ªán giao d·ªãch ‚Äì d√πng ƒë·ªÉ ph√¢n t√≠ch theo th·ªùi gian, m√πa v·ª•                            |
| UnitPrice    | Gi√° c·ªßa m·ªôt ƒë∆°n v·ªã s·∫£n ph·∫©m (t√≠nh theo b·∫£ng Anh ‚Äì GBP)                                              |
| CustomerID   | ID c·ªßa kh√°ch h√†ng ‚Äì gi√∫p ph√¢n t√≠ch h√†nh vi t·ª´ng ng∆∞·ªùi, nh∆∞ng c√≥ th·ªÉ b·ªã thi·∫øu (NaN)                 |
| Country      | Qu·ªëc gia c·ªßa kh√°ch h√†ng ‚Äì c√≥ th·ªÉ d√πng cho ph√¢n t√≠ch theo khu v·ª±c, th·ªã tr∆∞·ªùng                        |

Step 2 Initial Data Analysis
df.head(10)
df.info()
## M√¥ t·∫£ Dataset

B·ªô d·ªØ li·ªáu n√†y bao g·ªìm **8 c·ªôt** v√† **541.909 d√≤ng** d·ªØ li·ªáu.

| C·ªôt          | Ki·ªÉu d·ªØ li·ªáu   | M√¥ t·∫£ chi ti·∫øt                                                                                                 |
|--------------|----------------|---------------------------------------------------------------------------------------------------------------|
| InvoiceNo    | object (chu·ªói) | S·ªë h√≥a ƒë∆°n cho m·ªói giao d·ªãch. M·ªôt h√≥a ƒë∆°n c√≥ th·ªÉ ch·ª©a nhi·ªÅu s·∫£n ph·∫©m kh√°c nhau.                               |
| StockCode    | object (chu·ªói) | M√£ s·∫£n ph·∫©m c·ªßa t·ª´ng m·∫∑t h√†ng.                                                                                |
| Description  | object (chu·ªói) | M√¥ t·∫£ s·∫£n ph·∫©m. C√≥ gi√° tr·ªã thi·∫øu: ch·ªâ c√≥ **540.455** d√≤ng h·ª£p l·ªá tr√™n **541.909** d√≤ng.                       |
| Quantity     | int            | S·ªë l∆∞·ª£ng m·∫∑t h√†ng ƒë∆∞·ª£c mua trong m·ªói giao d·ªãch.                                                               |
| InvoiceDate  | datetime       | Ng√†y gi·ªù th·ª±c hi·ªán giao d·ªãch.                                                                                 |
| UnitPrice    | float          | ƒê∆°n gi√° c·ªßa t·ª´ng s·∫£n ph·∫©m.                                                                                    |
| CustomerID   | float          | M√£ ƒë·ªãnh danh kh√°ch h√†ng. C√≥ nhi·ªÅu gi√° tr·ªã b·ªã thi·∫øu: ch·ªâ c√≥ **406.829** d√≤ng h·ª£p l·ªá tr√™n **541.909** d√≤ng.    |
| Country      | object (chu·ªói) | Qu·ªëc gia n∆°i di·ªÖn ra m·ªói giao d·ªãch.                                                                            |

---

### Nh·∫≠n ƒë·ªãnh ban ƒë·∫ßu:

- C√≥ **gi√° tr·ªã thi·∫øu** trong c·ªôt **Description** v√† **CustomerID**, c·∫ßn ƒë∆∞·ª£c x·ª≠ l√Ω khi l√†m s·∫°ch d·ªØ li·ªáu.
- C·ªôt **InvoiceDate** ƒë√£ ·ªü ƒë·ªãnh d·∫°ng `datetime`, thu·∫≠n ti·ªán cho ph√¢n t√≠ch chu·ªói th·ªùi gian.
- M·ªôt kh√°ch h√†ng c√≥ th·ªÉ c√≥ nhi·ªÅu giao d·ªãch kh√°c nhau, ƒë∆∞·ª£c x√°c ƒë·ªãnh b·∫±ng vi·ªác l·∫∑p l·∫°i **CustomerID** trong nhi·ªÅu d√≤ng.

I will go deeper to understand insight of dataset
df.describe().T
df.describe(include='object').T
## K·∫øt lu·∫≠n Ph√¢n t√≠ch S∆° b·ªô (Preliminary Data Analysis)

### Quantity
- Trung b√¨nh: **9.55** s·∫£n ph·∫©m/giao d·ªãch.
- C√≥ gi√° tr·ªã √¢m (tr·∫£ h√†ng, ho√†n ƒë∆°n) ‚Äì c·∫ßn x·ª≠ l√Ω.
- Kho·∫£ng gi√° tr·ªã: **-80,995** ƒë·∫øn **80,995**.
- D·ªØ li·ªáu ph√¢n t√°n m·∫°nh (standard deviation l·ªõn), c√≥ outliers ‚Üí c·∫ßn x·ª≠ l√Ω.
  
---

### UnitPrice
- Trung b√¨nh: **4.61** GBP/s·∫£n ph·∫©m.
- C√≥ gi√° tr·ªã √¢m v√† r·∫•t cao (t·ª´ **-11,062.06** ƒë·∫øn **38,970**) ‚Üí nghi ng·ªù l·ªói nh·∫≠p li·ªáu ‚Üí c·∫ßn ki·ªÉm tra.
- Ph√¢n ph·ªëi l·ªách, c√≥ outliers l·ªõn.

---

### CustomerID
- C√≥ **406,829** d√≤ng h·ª£p l·ªá tr√™n **541,909** ‚Üí nhi·ªÅu gi√° tr·ªã thi·∫øu ‚Üí c·∫ßn x·ª≠ l√Ω.
- Kho·∫£ng gi√° tr·ªã ID: **12,346** ƒë·∫øn **18,287**.
- D√πng ƒë·ªÉ ph√¢n t√≠ch h√†nh vi kh√°ch h√†ng.

---

### InvoiceNo
- C√≥ **25,900** m√£ h√≥a ƒë∆°n kh√°c nhau ‚Üí ƒë·∫°i di·ªán cho **25,900 giao d·ªãch**.
- M√£ h√≥a ƒë∆°n xu·∫•t hi·ªán nhi·ªÅu nh·∫•t: **573585** (l·∫∑p l·∫°i **1,114** l·∫ßn) ‚Üí kh·∫£ nƒÉng l√† ƒë∆°n h√†ng l·ªõn ho·∫∑c nhi·ªÅu m·∫∑t h√†ng trong 1 giao d·ªãch.

---

### StockCode
- C√≥ **4,070** m√£ s·∫£n ph·∫©m kh√°c nhau.
- M√£ s·∫£n ph·∫©m ph·ªï bi·∫øn nh·∫•t: **85123A** (xu·∫•t hi·ªán **2,313** l·∫ßn).

---

### Description
- C√≥ **4,223** m√¥ t·∫£ s·∫£n ph·∫©m kh√°c nhau.
- M√¥ t·∫£ ph·ªï bi·∫øn nh·∫•t: **"WHITE HANGING HEART T-LIGHT HOLDER"** (xu·∫•t hi·ªán **2,369** l·∫ßn).
- C√≥ m·ªôt s·ªë gi√° tr·ªã thi·∫øu ‚Üí c·∫ßn x·ª≠ l√Ω khi l√†m s·∫°ch d·ªØ li·ªáu.

---

### Country
- G·ªìm **38** qu·ªëc gia.
- ~**91.4%** giao d·ªãch ƒë·∫øn t·ª´ **United Kingdom** ‚Üí th·ªã tr∆∞·ªùng ch√≠nh.

---

üëâ **T√≥m l·∫°i:**
- C·∫ßn l√†m s·∫°ch d·ªØ li·ªáu: lo·∫°i b·ªè/ƒëi·ªÅu ch·ªânh gi√° tr·ªã √¢m, x·ª≠ l√Ω missing values.
- C·∫©n th·∫≠n v·ªõi outliers ·ªü c·ªôt **Quantity** v√† **UnitPrice**.
- Khai th√°c th√™m ph√¢n t√≠ch theo **qu·ªëc gia**, **kh√°ch h√†ng**, **m√£ h√≥a ƒë∆°n**.

Step 3 | Data Cleaning & Transformation
#Handling Missing Values
#t√≠nh % c·ªßa missing values cho m·ªói c·ªôt
missing_values = df.isnull().sum()
percentage_missing = (missing_values[missing_values>0]/df.shape[0])*100
#sort values
percentage_missing.sort_values(ascending = True , inplace = True)
#v·∫Ω chart ƒë·ªÉ d·ªÖ nh√¨n c√≥ bao nhi√™u % missing value
fig, ax = plt.subplots(figsize=(15, 4))
ax.barh(percentage_missing.index, percentage_missing, color='#ff6200')
#hi·ªÉn th·ªã c√°c gi√° tr·ªã v√† ch·ªâ s·ªë
for i, (value, name) in enumerate(zip(percentage_missing, percentage_missing.index)):
    ax.text(value + 0.2, i, f"{value:.1f}%", ha='left', va='center', fontweight='black', fontsize=18, color='red')
#set limit
ax.set_xlim([0, 40])

#add title cho chart
plt.title("% Missing Values", fontweight='bold', fontsize=22)
plt.xlabel('Percentages (%)', fontsize=16)
plt.show()
## Chi·∫øn l∆∞·ª£c X·ª≠ l√Ω Gi√° tr·ªã Thi·∫øu (Handling Missing Values Strategy)

### *CustomerID* (thi·∫øu ~24.9%)
- C·ªôt **CustomerID** r·∫•t quan tr·ªçng, ƒë·∫∑c bi·ªát cho ph√¢n t√≠ch ph√¢n kh√∫c kh√°ch h√†ng (**customer segmentation**).  
- Tuy nhi√™n, c√≥ ƒë·∫øn **24.9% gi√° tr·ªã b·ªã thi·∫øu** ‚Üí Kh√¥ng th·ªÉ x√≥a b·ªè ho√†n to√†n c·ªôt n√†y, v√¨ s·∫Ω m·∫•t d·ªØ li·ªáu quan tr·ªçng.
- C·∫ßn √°p d·ª•ng chi·∫øn l∆∞·ª£c x·ª≠ l√Ω ph√π h·ª£p ƒë·ªÉ ƒëi·ªÅn gi√° tr·ªã thi·∫øu (v√≠ d·ª•: g√°n ID t·∫°m th·ªùi, ho·∫∑c ph√¢n t√≠ch ri√™ng c√°c giao d·ªãch kh√¥ng c√≥ CustomerID).

---

### *Description* (thi·∫øu ~0.3%)
- C·ªôt **Description** c√≥ t·ª∑ l·ªá thi·∫øu th·∫•p (**0.3%**), nh∆∞ng v·∫´n c·∫ßn x·ª≠ l√Ω ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ho√†n ch·ªânh.
- ƒê·∫∑c bi·ªát, ph√°t hi·ªán **s·ª± kh√¥ng nh·∫•t qu√°n gi·ªØa StockCode v√† Description**:  
  - C√πng m·ªôt StockCode, nh∆∞ng Description kh√°c nhau ‚Üí ƒê√¢y l√† v·∫•n ƒë·ªÅ ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu c·∫ßn x·ª≠ l√Ω (v√≠ d·ª•: ki·ªÉm tra/chu·∫©n h√≥a t√™n s·∫£n ph·∫©m).
- C·∫ßn r√† so√°t v√† chu·∫©n h√≥a c·ªôt **Description** ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n.

---

üëâ **T√≥m l·∫°i:**
- *CustomerID* c·∫ßn x·ª≠ l√Ω gi√° tr·ªã thi·∫øu thay v√¨ x√≥a b·ªè.
- *Description* c·∫ßn x·ª≠ l√Ω thi·∫øu gi√° tr·ªã v√† ki·ªÉm tra/chu·∫©n h√≥a n·ªôi dung ƒë·ªÉ tr√°nh sai s√≥t trong ph√¢n t√≠ch.

#Tr√≠ch xu·∫•t nh·ªØng d·ªØ li·ªáu b·ªã thi·∫øu CustomerID ho·∫∑c Description
df[df['CustomerID'].isnull() | df['Description'].isnull()].head()
#X√≥a nh·ªØng d·ªØ li·ªáu c·ªßa CustomerID v√† Description
df = df.dropna(subset=['CustomerID', 'Description'])
#Ki·ªÉm tra
df.isnull().sum().sum()

Step 3.2 Handling Duplicates
# t√¨m c√°c value duplicate
duplicate_rows = df[df.duplicated(keep=False)]

# s·∫Øp x·∫øp l·∫°i data
duplicate_rows_sorted = duplicate_rows.sort_values(by=['InvoiceNo', 'StockCode', 'Description', 'CustomerID', 'Quantity'])

duplicate_rows_sorted.head(10)
df.duplicated().sum()
#x√≥a tr·ª±c ti·∫øp value duplicates tr√™n df k c·∫ßn g√°n l·∫°i
df.drop_duplicates(inplace=True)
#ƒë·∫øm s·ªë d√≤ng sau khi x√≥a
df.shape[0]
Step 3.3 | Treating Cancelled Transactions
## Ph√¢n t√≠ch Giao d·ªãch B·ªã H·ªßy (Cancelled Transactions Analysis)

ƒê·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ **h√†nh vi v√† s·ªü th√≠ch c·ªßa kh√°ch h√†ng**, ch√∫ng ta c·∫ßn xem x√©t ƒë·∫øn c√°c **giao d·ªãch ƒë√£ b·ªã h·ªßy**.

- **B∆∞·ªõc 1**: X√°c ƒë·ªãnh c√°c giao d·ªãch b·ªã h·ªßy b·∫±ng c√°ch l·ªçc nh·ªØng d√≤ng m√† c·ªôt **InvoiceNo** b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ **"C"**.
- **B∆∞·ªõc 2**: Ph√¢n t√≠ch c√°c giao d·ªãch b·ªã h·ªßy ƒë·ªÉ t√¨m ra **c√°c ƒë·∫∑c ƒëi·ªÉm ho·∫∑c xu h∆∞·ªõng chung**.

üëâ Vi·ªác ph√¢n t√≠ch n√†y gi√∫p kh√°m ph√° c√°c **m·∫´u h√†nh vi ph·ªï bi·∫øn** ho·∫∑c **l√Ω do ti·ªÅm ·∫©n d·∫´n ƒë·∫øn vi·ªác h·ªßy ƒë∆°n**, t·ª´ ƒë√≥ gi√∫p c·∫£i thi·ªán **tr·∫£i nghi·ªám kh√°ch h√†ng** v√† **qu·∫£n l√Ω r·ªßi ro** t·ªët h∆°n.

df['Transaction_Status'] = np.where(df['InvoiceNo'].astype(str).str.startswith('C'), 'Cancelled', 'Completed')

# Analyze the characteristics of these rows (considering the new column)
cancelled_transactions = df[df['Transaction_Status'] == 'Cancelled']
cancelled_transactions.describe().drop('CustomerID', axis=1).round(2)
## Th·ªëng k√™ Giao d·ªãch B·ªã H·ªßy (Cancelled Transactions Statistics)

| Th√¥ng s·ªë         | Quantity (S·ªë l∆∞·ª£ng tr·∫£)            | UnitPrice (ƒê∆°n gi√° s·∫£n ph·∫©m)             |
|------------------|-----------------------------------|------------------------------------------|
| **S·ªë d√≤ng**      | 8,872 giao d·ªãch                   | 8,872 s·∫£n ph·∫©m                           |
| **Trung b√¨nh**   | -30.77 s·∫£n ph·∫©m                   | ¬£18.90                                   |
| **ƒê·ªô l·ªách chu·∫©n** | 1,172.25 (ph√¢n t√°n l·ªõn)           | 445.19 (nhi·ªÅu s·∫£n ph·∫©m gi√° cao)          |
| **Gi√° tr·ªã nh·ªè nh·∫•t** | -80,995 s·∫£n ph·∫©m                 | ¬£0.01 (r·∫•t r·∫ª)                            |
| **25% th·∫•p nh·∫•t** | -6 s·∫£n ph·∫©m                       | ¬£1.45                                    |
| **Trung v·ªã (50%)** | -2 s·∫£n ph·∫©m                       | ¬£2.95                                    |
| **75% cao nh·∫•t** | -1 s·∫£n ph·∫©m                       | ¬£4.95                                    |
| **L·ªõn nh·∫•t**     | -1 s·∫£n ph·∫©m                       | ¬£38,970 (r·∫•t ƒë·∫Øt)                         |

---

üëâ **Nh·∫≠n x√©t nhanh:**
- C·ªôt **Quantity** c√≥ gi√° tr·ªã √¢m l·ªõn (ƒë·∫∑c tr∆∞ng c·ªßa giao d·ªãch ho√†n tr·∫£), ph√¢n t√°n m·∫°nh.
- **UnitPrice** c√≥ kho·∫£ng gi√° r·∫•t r·ªông, t·ª´ gi√° r·∫•t r·∫ª (**¬£0.01**) ƒë·∫øn r·∫•t cao (**¬£38,970**), c·∫ßn xem x√©t c√°c s·∫£n ph·∫©m c√≥ gi√° b·∫•t th∆∞·ªùng.


## Nh·∫≠n x√©t v·ªÅ Giao d·ªãch B·ªã H·ªßy

- T·∫•t c·∫£ c√°c gi√° tr·ªã **Quantity** trong c√°c giao d·ªãch b·ªã h·ªßy ƒë·ªÅu l√† **s·ªë √¢m**, cho th·∫•y ƒë√¢y th·ª±c s·ª± l√† c√°c **ƒë∆°n h√†ng ƒë√£ b·ªã h·ªßy ho·∫∑c ho√†n tr·∫£**.

- C·ªôt **UnitPrice** c√≥ **m·ª©c ƒë·ªô ph√¢n t√°n l·ªõn**, cho th·∫•y r·∫±ng c√°c giao d·ªãch b·ªã h·ªßy bao g·ªìm **nhi·ªÅu lo·∫°i s·∫£n ph·∫©m** ‚Äî t·ª´ **gi√° th·∫•p** ƒë·∫øn **gi√° cao**.

## Chi·∫øn l∆∞·ª£c X·ª≠ l√Ω C√°c Giao d·ªãch B·ªã H·ªßy

X√©t ƒë·∫øn m·ª•c ti√™u c·ªßa d·ª± √°n l√† **ph√¢n c·ª•m kh√°ch h√†ng** d·ª±a tr√™n **h√†nh vi v√† s·ªü th√≠ch mua s·∫Øm**, ƒë·ªìng th·ªùi ti·∫øn t·ªõi x√¢y d·ª±ng **h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m**, vi·ªác **hi·ªÉu r√µ xu h∆∞·ªõng h·ªßy ƒë∆°n h√†ng** c·ªßa kh√°ch h√†ng l√† **r·∫•t quan tr·ªçng**.

Do ƒë√≥, **chi·∫øn l∆∞·ª£c** s·∫Ω l√†:
- **Gi·ªØ l·∫°i** c√°c giao d·ªãch b·ªã h·ªßy trong t·∫≠p d·ªØ li·ªáu.
- **ƒê√°nh d·∫•u ri√™ng bi·ªát** c√°c giao d·ªãch n√†y ƒë·ªÉ ph·ª•c v·ª• cho c√°c ph√¢n t√≠ch sau n√†y.

---

### L·ª£i √≠ch c·ªßa chi·∫øn l∆∞·ª£c:

- **C·∫£i thi·ªán qu√° tr√¨nh ph√¢n c·ª•m kh√°ch h√†ng**:  
  Nh·ªù ƒë∆∞a v√†o c√°c xu h∆∞·ªõng v√† m√¥ h√¨nh xu·∫•t hi·ªán trong d·ªØ li·ªáu h·ªßy ƒë∆°n, gi√∫p ph√°t hi·ªán c√°c **h√†nh vi ƒë·∫∑c tr∆∞ng** c·ªßa kh√°ch h√†ng.

- **N√¢ng cao ch·∫•t l∆∞·ª£ng h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m**:  
  H·ªá th·ªëng c√≥ th·ªÉ **tr√°nh ƒë·ªÅ xu·∫•t c√°c s·∫£n ph·∫©m d·ªÖ b·ªã h·ªßy**, t·ª´ ƒë√≥ n√¢ng cao ƒë·ªô ch√≠nh x√°c v√† gi√° tr·ªã c·ªßa c√°c g·ª£i √Ω d√†nh cho kh√°ch h√†ng.

cancelled_percentage = (cancelled_transactions.shape[0] / df.shape[0]) * 100

# Printing the percentage of cancelled transactions
print(f"The percentage of cancelled transactions in the dataset is: {cancelled_percentage:.2f}%")
# t√¨m ki·∫øm gi√° tr·ªã duy nh·∫•t c·ªßa Stockcode
unique_stock_codes = df['StockCode'].nunique()

print(f"The number of unique stock codes in the dataset is: {unique_stock_codes}")
# T√¨m 10 m√£ s·∫£n ph·∫©m (StockCode) xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
top_10_stock_codes = df['StockCode'].value_counts(normalize=True).head(10) * 100

# V·∫Ω bi·ªÉu ƒë·ªì ngang (bar chart) cho 10 m√£ s·∫£n ph·∫©m xu·∫•t hi·ªán nhi·ªÅu nh·∫•t
plt.figure(figsize=(12, 5))
top_10_stock_codes.plot(kind='barh', color='orange')

# Th√™m nh√£n ph·∫ßn trƒÉm t·∫ßn su·∫•t tr√™n c√°c thanh bi·ªÉu ƒë·ªì
for index, value in enumerate(top_10_stock_codes):
    plt.text(value, index+0.2, f'{value:.2f}%', fontsize=10)

plt.title('Top 10 M√£ S·∫£n Ph·∫©m Xu·∫•t Hi·ªán Nhi·ªÅu Nh·∫•t')
plt.xlabel('T·∫ßn Su·∫•t (%)')
plt.ylabel('M√£ S·∫£n Ph·∫©m')
plt.gca().invert_yaxis()
plt.show()
## Ph√¢n t√≠ch D·ªØ li·ªáu S·∫£n ph·∫©m

- Dataset ch·ª©a **3,684 m√£ s·∫£n ph·∫©m duy nh·∫•t**, cho th·∫•y c·ª≠a h√†ng b√°n l·∫ª n√†y c√≥ **danh m·ª•c s·∫£n ph·∫©m ƒëa d·∫°ng**.  
  ƒêi·ªÅu n√†y gi√∫p **vi·ªác ph√¢n c·ª•m kh√°ch h√†ng tr·ªü n√™n ƒëa d·∫°ng v√† chi ti·∫øt h∆°n**, v√¨ m·ªói nh√≥m kh√°ch h√†ng th∆∞·ªùng c√≥ xu h∆∞·ªõng y√™u th√≠ch nh·ªØng lo·∫°i s·∫£n ph·∫©m ri√™ng.

---

- Bi·ªÉu ƒë·ªì cho th·∫•y **10 s·∫£n ph·∫©m ƒë∆∞·ª£c mua nhi·ªÅu nh·∫•t**, ƒë·∫°i di·ªán cho nh·ªØng **m√£ s·∫£n ph·∫©m ph·ªï bi·∫øn** ‚Äì t·ª©c l√† nh·ªØng s·∫£n ph·∫©m m√† kh√°ch h√†ng th∆∞·ªùng xuy√™n mua nh·∫•t.  
  ƒê√¢y l√† **c√°c s·∫£n ph·∫©m c·ªët l√µi** gi√∫p ph√¢n lo·∫°i h√†nh vi mua s·∫Øm c·ªßa kh√°ch h√†ng.

---

- Tuy nhi√™n, ph·∫ßn l·ªõn c√°c m√£ s·∫£n ph·∫©m c√≥ t·ª´ **5 ƒë·∫øn 6 k√Ω t·ª±**, nh∆∞ng t·ªìn t·∫°i m·ªôt s·ªë **ngo·∫°i l·ªá** nh∆∞ m√£ **'POST'**.  
  Nh·ªØng m√£ n√†y **c√≥ th·ªÉ ƒë·∫°i di·ªán cho d·ªãch v·ª•** ho·∫∑c c√°c giao d·ªãch kh√¥ng ph·∫£i s·∫£n ph·∫©m (v√≠ d·ª•: **ph√≠ v·∫≠n chuy·ªÉn**).

---

### ƒê·ªÅ xu·∫•t x·ª≠ l√Ω:
- ƒê·ªÉ **gi·ªØ ƒë√∫ng tr·ªçng t√¢m c·ªßa d·ª± √°n** l√† **ph√¢n c·ª•m kh√°ch h√†ng d·ª±a tr√™n s·∫£n ph·∫©m ƒë√£ mua** v√† **x√¢y d·ª±ng h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m**, c·∫ßn:
  - **Ki·ªÉm tra v√† x·ª≠ l√Ω** c√°c m√£ b·∫•t th∆∞·ªùng nh∆∞ `'POST'` ƒë·ªÉ ƒë·∫£m b·∫£o **t√≠nh to√†n v·∫πn c·ªßa d·ªØ li·ªáu**.
  - Xem x√©t **lo·∫°i b·ªè** ho·∫∑c **g·∫Øn nh√£n ri√™ng** c√°c m√£ n√†y ƒë·ªÉ tr√°nh ·∫£nh h∆∞·ªüng t·ªõi k·∫øt qu·∫£ ph√¢n c·ª•m v√† ƒë·ªÅ xu·∫•t.

To delve deeper into identifying these anomalies, let's explore the frequency of the number of numeric characters in the stock codes, which can provide insights into the nature of these unusual entries:
# T√¨m s·ªë l∆∞·ª£ng k√Ω t·ª± s·ªë trong m·ªói m√£ s·∫£n ph·∫©m duy nh·∫•t (StockCode)
unique_stock_codes = df['StockCode'].unique()
numeric_char_counts_in_unique_codes = pd.Series(unique_stock_codes).apply(lambda x: sum(c.isdigit() for c in str(x))).value_counts()

print("Value counts of numeric character frequencies in unique stock codes:")
print("*-*"*22)
print(numeric_char_counts_in_unique_codes)
## Ph√¢n t√≠ch ƒê·ªãnh d·∫°ng M√£ S·∫£n ph·∫©m

- **Ph·∫ßn l·ªõn** c√°c m√£ s·∫£n ph·∫©m duy nh·∫•t (**3,676 tr√™n 3,684**) c√≥ **ch√≠nh x√°c 5 k√Ω t·ª± s·ªë**.  
  ƒê√¢y d∆∞·ªùng nh∆∞ l√† **ƒë·ªãnh d·∫°ng ti√™u chu·∫©n** ƒë·ªÉ bi·ªÉu di·ªÖn m√£ s·∫£n ph·∫©m trong t·∫≠p d·ªØ li·ªáu n√†y.

---

### Nh·ªØng ngo·∫°i l·ªá c·∫ßn ch√∫ √Ω:
- **7 m√£** kh√¥ng ch·ª©a **b·∫•t k·ª≥ k√Ω t·ª± s·ªë n√†o**.
- **1 m√£** ch·ªâ ch·ª©a **duy nh·∫•t 1 k√Ω t·ª± s·ªë**.

---

üëâ Nh·ªØng m√£ n√†y **l·ªách kh·ªèi ƒë·ªãnh d·∫°ng chu·∫©n** v√† c·∫ßn ƒë∆∞·ª£c ki·ªÉm tra k·ªπ l∆∞·ª°ng ƒë·ªÉ:
- Hi·ªÉu r√µ b·∫£n ch·∫•t th·ª±c s·ª± c·ªßa ch√∫ng.
- X√°c ƒë·ªãnh xem ƒë√¢y c√≥ ph·∫£i l√† **giao d·ªãch s·∫£n ph·∫©m h·ª£p l·ªá** hay kh√¥ng, hay ch·ªâ l√† **d·ªãch v·ª•, ph√≠, ho·∫∑c l·ªói d·ªØ li·ªáu**.

---

### Ti·∫øp theo:
- X√°c ƒë·ªãnh danh s√°ch c√°c m√£ s·∫£n ph·∫©m c√≥ **0 ho·∫∑c 1 k√Ω t·ª± s·ªë** ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt h∆°n.

# T√¨m v√† in ra c√°c m√£ s·∫£n ph·∫©m c√≥ 0 ho·∫∑c 1 k√Ω t·ª± s·ªë
anomalous_stock_codes = [code for code in unique_stock_codes if sum(c.isdigit() for c in str(code)) in (0, 1)]

print("Anomalous stock codes:")
print("-"*22)
for code in anomalous_stock_codes:
    print(code)
Let's calculate the percentage of records with these anomalous stock codes:
# Calculating the percentage of records with these stock codes
percentage_anomalous = (df['StockCode'].isin(anomalous_stock_codes).sum() / len(df)) * 100

# Printing the percentage
print(f"The percentage of records with anomalous stock codes in the dataset is: {percentage_anomalous:.2f}%")
## K·∫øt lu·∫≠n

D·ª±a tr√™n ph√¢n t√≠ch, ta nh·∫≠n th·∫•y:

- Ch·ªâ m·ªôt **t·ª∑ l·ªá r·∫•t nh·ªè** c·ªßa c√°c b·∫£n ghi (**0.48%**) c√≥ m√£ s·∫£n ph·∫©m **b·∫•t th∆∞·ªùng** ‚Äì t·ª©c l√† nh·ªØng m√£ **kh√¥ng tu√¢n theo ƒë·ªãnh d·∫°ng chu·∫©n** ƒë∆∞·ª£c quan s√°t trong ph·∫ßn l·ªõn d·ªØ li·ªáu.
- C·ª• th·ªÉ, c√°c m√£ b·∫•t th∆∞·ªùng n√†y ch·ªâ chi·∫øm **8 tr√™n t·ªïng s·ªë 3,684 m√£ s·∫£n ph·∫©m duy nh·∫•t**.

---

### ƒê·∫∑c ƒëi·ªÉm c·ªßa c√°c m√£ b·∫•t th∆∞·ªùng:
- D∆∞·ªùng nh∆∞ **kh√¥ng ƒë·∫°i di·ªán cho s·∫£n ph·∫©m th·ª±c t·∫ø**.
- C√≥ th·ªÉ l√† c√°c giao d·ªãch ph·ª• tr·ª£ nh∆∞:
  - `"BANK CHARGES"` (ph√≠ ng√¢n h√†ng)
  - `"POST"` (c√≥ th·ªÉ l√† **ph√≠ v·∫≠n chuy·ªÉn**), v.v.

---

### Khuy·∫øn ngh·ªã:
- V√¨ c√°c m√£ n√†y **kh√¥ng ph·∫£i s·∫£n ph·∫©m th·ª±c** v√† ch·ªâ chi·∫øm **t·ª∑ l·ªá r·∫•t nh·ªè** trong to√†n b·ªô d·ªØ li·ªáu, vi·ªác **gi·ªØ l·∫°i** trong ph√¢n t√≠ch c√≥ th·ªÉ:
  - G√¢y nhi·ªÖu cho d·ªØ li·ªáu.
  - L√†m **sai l·ªách k·∫øt qu·∫£ ph√¢n c·ª•m** kh√°ch h√†ng ho·∫∑c **l√†m gi·∫£m ƒë·ªô ch√≠nh x√°c c·ªßa h·ªá th·ªëng g·ª£i √Ω**.

‚û°Ô∏è **N√™n lo·∫°i b·ªè ho·∫∑c g·∫Øn nh√£n ri√™ng** c√°c m√£ n√†y ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c v√† ch·∫•t l∆∞·ª£ng c·ªßa ph√¢n t√≠ch.

## Chi·∫øn l∆∞·ª£c X·ª≠ l√Ω C√°c M√£ S·∫£n ph·∫©m B·∫•t Th∆∞·ªùng

X√©t trong **b·ªëi c·∫£nh c·ªßa d·ª± √°n**, v·ªõi m·ª•c ti√™u l√† **ph√¢n c·ª•m kh√°ch h√†ng d·ª±a tr√™n h√†nh vi mua s·∫£n ph·∫©m** v√† **x√¢y d·ª±ng h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m**, th√¨ vi·ªác **lo·∫°i b·ªè c√°c b·∫£n ghi c√≥ m√£ s·∫£n ph·∫©m b·∫•t th∆∞·ªùng** l√† m·ªôt l·ª±a ch·ªçn **h·ª£p l√Ω v√† c·∫ßn thi·∫øt**.

---

### L√Ω do:
- Gi√∫p qu√° tr√¨nh ph√¢n t√≠ch **t·∫≠p trung ho√†n to√†n v√†o c√°c giao d·ªãch s·∫£n ph·∫©m th·ª±c t·∫ø**.
- Lo·∫°i b·ªè c√°c giao d·ªãch kh√¥ng li√™n quan nh∆∞:
  - `"POST"` (ph√≠ v·∫≠n chuy·ªÉn)
  - `"BANK CHARGES"` (ph√≠ ng√¢n h√†ng)
  - V√† c√°c m√£ kh√¥ng tu√¢n theo ƒë·ªãnh d·∫°ng chu·∫©n.
- ƒê·∫£m b·∫£o k·∫øt qu·∫£ ph√¢n t√≠ch v√† m√¥ h√¨nh ƒë∆∞·ª£c x√¢y d·ª±ng **ch√≠nh x√°c, √Ω nghƒ©a v√† ph·∫£n √°nh ƒë√∫ng h√†nh vi kh√°ch h√†ng**.

---

### Chi·∫øn l∆∞·ª£c th·ª±c hi·ªán:
üëâ **L·ªçc v√† lo·∫°i b·ªè** kh·ªèi t·∫≠p d·ªØ li·ªáu c√°c d√≤ng c√≥ **m√£ s·∫£n ph·∫©m b·∫•t th∆∞·ªùng** tr∆∞·ªõc khi th·ª±c hi·ªán c√°c b∆∞·ªõc ph√¢n t√≠ch s√¢u h∆°n v√† x√¢y d·ª±ng m√¥ h√¨nh.

---

N·∫øu b·∫°n mu·ªën m√¨nh vi·∫øt **code Python m·∫´u** ƒë·ªÉ th·ª±c hi·ªán b∆∞·ªõc l·ªçc n√†y, h√£y cho m√¨nh bi·∫øt nh√©! üöÄ

# Lo·∫°i b·ªè nh·ªØng m√£ kh√°c th∆∞·ªùng
df = df[~df['StockCode'].isin(anomalous_stock_codes)]
# L·∫•y s·ªë d√≤ng trong dataframe
df.shape[0]
Step 3.5 | Cleaning Description Column
## K·∫ø ho·∫°ch Ph√¢n t√≠ch M√¥ t·∫£ S·∫£n ph·∫©m (Description)

Tr∆∞·ªõc ti√™n, t√¥i s·∫Ω **t√≠nh s·ªë l·∫ßn xu·∫•t hi·ªán** c·ªßa t·ª´ng **m√¥ t·∫£ s·∫£n ph·∫©m (Description)** trong t·∫≠p d·ªØ li·ªáu.

Sau ƒë√≥, t√¥i s·∫Ω **v·∫Ω bi·ªÉu ƒë·ªì** cho **30 m√¥ t·∫£ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t**.

Bi·ªÉu ƒë·ªì n√†y s·∫Ω gi√∫p **tr·ª±c quan h√≥a r√µ r√†ng** c√°c **m√¥ t·∫£ s·∫£n ph·∫©m ph·ªï bi·∫øn nh·∫•t** trong t·∫≠p d·ªØ li·ªáu.

# Calculate the occurrence of each unique description and sort them
description_counts = df['Description'].value_counts()

# Get the top 30 descriptions
top_30_descriptions = description_counts[:30]

# Plotting
plt.figure(figsize=(12,8))
plt.barh(top_30_descriptions.index[::-1], top_30_descriptions.values[::-1], color='#ff6200')

# Adding labels and title
plt.xlabel('Number of Occurrences')
plt.ylabel('Description')
plt.title('Top 30 Most Frequent Descriptions')

# Show the plot
plt.show()
## Nh·∫≠n x√©t v·ªÅ C·ªôt M√¥ t·∫£ S·∫£n ph·∫©m (Descriptions)

- C√°c m√¥ t·∫£ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t **ch·ªß y·∫øu l√† ƒë·ªì gia d·ª•ng**, ƒë·∫∑c bi·ªát l√† c√°c s·∫£n ph·∫©m li√™n quan ƒë·∫øn:
  - **D·ª•ng c·ª• nh√† b·∫øp**
  - **T√∫i ƒë·ª±ng c∆°m tr∆∞a**
  - **ƒê·ªì trang tr√≠**

---

- **ƒê√°ng ch√∫ √Ω**, t·∫•t c·∫£ c√°c m√¥ t·∫£ ƒë·ªÅu ƒë∆∞·ª£c **vi·∫øt hoa (UPPERCASE)** ‚Äî ƒëi·ªÅu n√†y c√≥ th·ªÉ l√† **quy chu·∫©n ƒë·ªãnh d·∫°ng** khi nh·∫≠p d·ªØ li·ªáu v√†o c∆° s·ªü d·ªØ li·ªáu.

---

- Tuy nhi√™n, **x√©t ƒë·∫øn nh·ªØng b·∫•t th∆∞·ªùng v√† thi·∫øu nh·∫•t qu√°n ƒë√£ g·∫∑p trong t·∫≠p d·ªØ li·ªáu**, c·∫ßn th·ª±c hi·ªán ki·ªÉm tra k·ªπ h∆°n ƒë·ªÉ x√°c ƒë·ªãnh:
  - Li·ªáu c√≥ t·ªìn t·∫°i m√¥ t·∫£ n√†o ƒë∆∞·ª£c nh·∫≠p b·∫±ng **ch·ªØ th∆∞·ªùng (lowercase)** ho·∫∑c **pha tr·ªôn gi·ªØa ch·ªØ hoa v√† ch·ªØ th∆∞·ªùng** hay kh√¥ng.
  - N·∫øu c√≥, c·∫ßn x·ª≠ l√Ω ƒë·ªìng nh·∫•t ƒë·ªÉ ƒë·∫£m b·∫£o **t√≠nh nh·∫•t qu√°n d·ªØ li·ªáu** cho ph√¢n t√≠ch sau n√†y.

# L·∫•y c√°c m√¥ t·∫£ duy nh·∫•t trong c·ªôt 'Description'
unique_descriptions = df['Description'].dropna().unique()

# L·ªçc c√°c m√¥ t·∫£ ch·ª©a √≠t nh·∫•t m·ªôt k√Ω t·ª± th∆∞·ªùng (lowercase)
lowercase_descriptions = [desc for desc in unique_descriptions if any(char.islower() for char in desc)]

# In c√°c m√¥ t·∫£ b·∫•t th∆∞·ªùng
print("‚ö†Ô∏è C√°c m√¥ t·∫£ ch·ª©a k√Ω t·ª± th∆∞·ªùng (kh√¥ng vi·∫øt hoa ho√†n to√†n):")
print("-" * 60)
for desc in lowercase_descriptions:
    print(desc)

# L·ªçc to√†n b·ªô c√°c d√≤ng trong df c√≥ m√¥ t·∫£ thu·ªôc danh s√°ch n√†y
rows_with_lowercase = df[df['Description'].isin(lowercase_descriptions)]

# Th·ªëng k√™
print(f"\nüßæ T·ªïng s·ªë d√≤ng d·ªØ li·ªáu c√≥ m√¥ t·∫£ vi·∫øt kh√¥ng ho√†n to√†n hoa: {rows_with_lowercase.shape[0]}")
print("\nüìã M·ªôt s·ªë v√≠ d·ª•:")
print(rows_with_lowercase[['InvoiceNo', 'StockCode', 'Description']].drop_duplicates().head(10))

## Nh·∫≠n ƒë·ªãnh:

Sau khi xem x√©t c√°c m√¥ t·∫£ s·∫£n ph·∫©m c√≥ ch·ª©a **ch·ªØ th∆∞·ªùng**, c√≥ th·ªÉ th·∫•y r·∫±ng **m·ªôt s·ªë m·ª•c kh√¥ng ph·∫£i l√† m√¥ t·∫£ s·∫£n ph·∫©m th·ª±c t·∫ø**, v√≠ d·ª• nh∆∞:
- `"Next Day Carriage"`
- `"High Resolution Image"`

Nh·ªØng m·ª•c n√†y d∆∞·ªùng nh∆∞ li√™n quan ƒë·∫øn **d·ªãch v·ª•** ho·∫∑c **th√¥ng tin ph·ª• tr·ª£**, **kh√¥ng ph·∫£i l√† s·∫£n ph·∫©m ƒë·ªÉ b√°n**.

üëâ Do ƒë√≥, c·∫ßn l∆∞u √Ω **l·ªçc b·ªè ho·∫∑c g·∫Øn nh√£n ri√™ng** c√°c m√¥ t·∫£ n√†y khi th·ª±c hi·ªán ph√¢n t√≠ch s·∫£n ph·∫©m v√† ph√¢n c·ª•m kh√°ch h√†ng.

## Chi·∫øn l∆∞·ª£c X·ª≠ l√Ω D·ªØ li·ªáu C·ªôt M√¥ t·∫£ S·∫£n ph·∫©m (Description)

### B∆∞·ªõc 1:
- **Lo·∫°i b·ªè** c√°c d√≤ng m√† m√¥ t·∫£ ch·ª©a **th√¥ng tin d·ªãch v·ª•** nh∆∞:
  - `"Next Day Carriage"`
  - `"High Resolution Image"`
- L√Ω do: ƒê√¢y **kh√¥ng ph·∫£i s·∫£n ph·∫©m th·ª±c t·∫ø** v√† **kh√¥ng ƒë√≥ng g√≥p gi√° tr·ªã** cho qu√° tr√¨nh ph√¢n c·ª•m ho·∫∑c h·ªá th·ªëng g·ª£i √Ω m√† ch√∫ng ta ƒëang x√¢y d·ª±ng.

---

### B∆∞·ªõc 2:
- V·ªõi c√°c m√¥ t·∫£ c√≤n l·∫°i c√≥ **ch·ªØ th∆∞·ªùng** ho·∫∑c **ki·ªÉu ch·ªØ pha tr·ªôn**, ti·∫øn h√†nh **chu·∫©n h√≥a t·∫•t c·∫£ v·ªÅ d·∫°ng ch·ªØ in hoa (UPPERCASE)** ƒë·ªÉ:
  - ƒê·∫£m b·∫£o **s·ª± ƒë·ªìng nh·∫•t trong d·ªØ li·ªáu**.
  - **Gi·∫£m kh·∫£ nƒÉng tr√πng l·∫∑p m√¥ t·∫£** do kh√°c bi·ªát ki·ªÉu ch·ªØ.

---

### L·ª£i √≠ch:
Vi·ªác th·ª±c hi·ªán chi·∫øn l∆∞·ª£c tr√™n s·∫Ω gi√∫p:
‚úÖ N√¢ng cao **ch·∫•t l∆∞·ª£ng t·∫≠p d·ªØ li·ªáu**.  
‚úÖ L√†m cho d·ªØ li·ªáu **ph√π h·ª£p h∆°n** cho giai ƒëo·∫°n ph√¢n t√≠ch v√† x√¢y d·ª±ng m√¥ h√¨nh c·ªßa d·ª± √°n.

# Danh s√°ch m√¥ t·∫£ kh√¥ng ph·∫£i s·∫£n ph·∫©m (d·ªãch v·ª•)
service_related_descriptions = ["Next Day Carriage", "High Resolution Image"]

# T√≠nh % d√≤ng ch·ª©a m√¥ t·∫£ d·ªãch v·ª•
service_related_percentage = (
    df[df['Description'].isin(service_related_descriptions)].shape[0]
    / df.shape[0]
) * 100

# In ph·∫ßn trƒÉm tr∆∞·ªõc khi x·ª≠ l√Ω
print(f"The percentage of records with service-related descriptions in the dataset is: {service_related_percentage:.2f}%")

# Xo√° c√°c d√≤ng c√≥ m√¥ t·∫£ d·ªãch v·ª•
df = df[~df['Description'].isin(service_related_descriptions)]

# Chu·∫©n h√≥a m√¥ t·∫£ c√≤n l·∫°i v·ªÅ vi·∫øt hoa
df['Description'] = df['Description'].str.upper()

df.shape[0]
Step 3.6 | Treating Zero Unit Prices
df['UnitPrice'].describe()
## Nh·∫≠n ƒë·ªãnh:

- **Gi√° tr·ªã nh·ªè nh·∫•t c·ªßa c·ªôt UnitPrice l√† 0**.  
  ƒêi·ªÅu n√†y cho th·∫•y c√≥ m·ªôt s·ªë giao d·ªãch c√≥ **ƒë∆°n gi√° b·∫±ng 0**, c√≥ th·ªÉ l√†:
  - **S·∫£n ph·∫©m ƒë∆∞·ª£c t·∫∑ng mi·ªÖn ph√≠**.
  - **L·ªói nh·∫≠p li·ªáu**.

---

### K·∫ø ho·∫°ch ph√¢n t√≠ch ti·∫øp theo:
- ƒê·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ **b·∫£n ch·∫•t c·ªßa c√°c giao d·ªãch 0 ƒë·ªìng**, c·∫ßn th·ª±c hi·ªán:
  - Ph√¢n t√≠ch chi ti·∫øt **c√°c d√≤ng c√≥ UnitPrice b·∫±ng 0**.
  - Xem x√©t **m√¥ t·∫£ s·∫£n ph·∫©m (Description)** t∆∞∆°ng ·ª©ng ƒë·ªÉ ph√°t hi·ªán xem c√≥ tu√¢n theo m·ªôt **m·∫´u (pattern) c·ª• th·ªÉ** n√†o kh√¥ng.

---

üëâ Vi·ªác n√†y s·∫Ω gi√∫p quy·∫øt ƒë·ªãnh:
- Li·ªáu c√≥ n√™n lo·∫°i b·ªè c√°c d√≤ng n√†y kh·ªèi ph√¢n t√≠ch.
- Hay c·∫ßn x·ª≠ l√Ω/ƒë√°nh d·∫•u theo m·ªôt c√°ch ƒë·∫∑c bi·ªát tr∆∞·ªõc khi ph√¢n c·ª•m v√† x√¢y d·ª±ng h·ªá th·ªëng g·ª£i √Ω.

df[df['UnitPrice']==0].describe()[['Quantity']]
## Nh·∫≠n ƒë·ªãnh v·ªÅ C·ªôt UnitPrice

- **S·ªë l∆∞·ª£ng giao d·ªãch c√≥ ƒë∆°n gi√° b·∫±ng 0** l√† kh√° √≠t: ch·ªâ **33 giao d·ªãch**.

- Tuy nhi√™n, nh·ªØng giao d·ªãch n√†y c√≥ **m·ª©c ƒë·ªô bi·∫øn ƒë·ªông l·ªõn v·ªÅ s·ªë l∆∞·ª£ng s·∫£n ph·∫©m**, c·ª• th·ªÉ:
  - Dao ƒë·ªông t·ª´ **1** ƒë·∫øn **12,540** s·∫£n ph·∫©m.
  - ƒê·ªô l·ªách chu·∫©n r·∫•t cao, cho th·∫•y m·ª©c ƒë·ªô ph√¢n t√°n m·∫°nh.

---

### T√°c ƒë·ªông ti·ªÅm ·∫©n:
- Vi·ªác **gi·ªØ l·∫°i c√°c giao d·ªãch n√†y trong qu√° tr√¨nh ph√¢n c·ª•m (clustering)** c√≥ th·ªÉ:
  - **G√¢y nhi·ªÖu d·ªØ li·ªáu**.
  - L√†m **sai l·ªách k·∫øt qu·∫£ nh·∫≠n di·ªán h√†nh vi kh√°ch h√†ng** m√† thu·∫≠t to√°n ph√¢n c·ª•m t·∫°o ra.

---

üëâ C·∫ßn c√¢n nh·∫Øc k·ªπ vi·ªác **lo·∫°i b·ªè ho·∫∑c x·ª≠ l√Ω ri√™ng bi·ªát** c√°c giao d·ªãch c√≥ ƒë∆°n gi√° b·∫±ng 0 ƒë·ªÉ ƒë·∫£m b·∫£o **ch·∫•t l∆∞·ª£ng ph√¢n t√≠ch v√† ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh**.

## Chi·∫øn l∆∞·ª£c X·ª≠ l√Ω Giao d·ªãch ƒê∆°n Gi√° 0

X√©t th·∫•y:
- **S·ªë l∆∞·ª£ng c√°c giao d·ªãch n√†y r·∫•t nh·ªè** (ch·ªâ 33 giao d·ªãch).
- **C√≥ kh·∫£ nƒÉng g√¢y nhi·ªÖu d·ªØ li·ªáu** v√† ·∫£nh h∆∞·ªüng ƒë·∫øn qu√° tr√¨nh ph√¢n t√≠ch.

üëâ **Chi·∫øn l∆∞·ª£c h·ª£p l√Ω** l√† **lo·∫°i b·ªè nh·ªØng giao d·ªãch n√†y kh·ªèi t·∫≠p d·ªØ li·ªáu**.

---

### L·ª£i √≠ch:
- Gi√∫p duy tr√¨ m·ªôt **t·∫≠p d·ªØ li·ªáu s·∫°ch v√† nh·∫•t qu√°n h∆°n**.
- ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë·∫ßu v√†o ph√π h·ª£p cho vi·ªác x√¢y d·ª±ng:
  - **M√¥ h√¨nh ph√¢n c·ª•m kh√°ch h√†ng (Clustering)**.
  - **H·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m (Recommendation System)**.
- TƒÉng ƒë·ªô **ch√≠nh x√°c** v√† **ƒë√°ng tin c·∫≠y** c·ªßa c√°c k·∫øt qu·∫£ ph√¢n t√≠ch v√† m√¥ h√¨nh.

---

‚úÖ ƒê√¢y l√† m·ªôt b∆∞·ªõc **ti·ªÅn x·ª≠ l√Ω c·∫ßn thi·∫øt** ƒë·ªÉ lo·∫°i b·ªè nhi·ªÖu v√† ƒë·∫£m b·∫£o ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho d·ª± √°n.

# X√≥a c√°c b·∫£n ghi c√≥ ƒë∆°n gi√° b·∫±ng 0 ƒë·ªÉ tr√°nh l·ªói nh·∫≠p li·ªáu ti·ªÅm ·∫©n.
df = df[df['UnitPrice'] > 0]
Step 3.7 | Outlier Treatment
## Nh·∫≠n ƒë·ªãnh v·ªÅ Vi·ªác X·ª≠ L√Ω Outlier trong Ph√¢n C·ª•m K-means

Trong ph√¢n c·ª•m **K-means**, thu·∫≠t to√°n r·∫•t nh·∫°y c·∫£m v·ªõi:
- **Quy m√¥ d·ªØ li·ªáu** (scale).
- S·ª± hi·ªán di·ªán c·ªßa **gi√° tr·ªã ngo·∫°i lai (outliers)**.

Nh·ªØng y·∫øu t·ªë n√†y c√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë√°ng k·ªÉ ƒë·∫øn **v·ªã tr√≠ c√°c t√¢m c·ª•m (centroids)**, t·ª´ ƒë√≥ d·∫´n ƒë·∫øn vi·ªác **ph√¢n c·ª•m sai l·ªách**.

---

### Tuy nhi√™n, trong b·ªëi c·∫£nh d·ª± √°n n√†y:
- M·ª•c ti√™u cu·ªëi c√πng l√† **hi·ªÉu h√†nh vi v√† s·ªü th√≠ch c·ªßa kh√°ch h√†ng** th√¥ng qua **K-means**.
- V√¨ v·∫≠y, vi·ªác x·ª≠ l√Ω **outlier** n√™n ƒë∆∞·ª£c th·ª±c hi·ªán **sau giai ƒëo·∫°n x√¢y d·ª±ng ƒë·∫∑c tr∆∞ng (feature engineering)**, khi d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·ªïng h·ª£p theo **kh√°ch h√†ng**.
- ·ªû giai ƒëo·∫°n hi·ªán t·∫°i, d·ªØ li·ªáu v·∫´n ·ªü **c·∫•p ƒë·ªô giao d·ªãch**.
- Vi·ªác lo·∫°i b·ªè **outlier qu√° s·ªõm** c√≥ th·ªÉ khi·∫øn ta m·∫•t ƒëi **nh·ªØng th√¥ng tin quan tr·ªçng**, v·ªën c√≥ th·ªÉ ƒë√≥ng vai tr√≤ **then ch·ªët trong ph√¢n kh√∫c kh√°ch h√†ng** v·ªÅ sau.

---

üëâ **K·∫øt lu·∫≠n:**
- **Ho√£n x·ª≠ l√Ω outlier** ·ªü giai ƒëo·∫°n n√†y.
- Ti·∫øp t·ª•c chuy·ªÉn sang b∆∞·ªõc ti·∫øp theo trong qu√° tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu.

---

üéØ Khi ho√†n t·∫•t **feature engineering** v√† c√≥ dataset **theo kh√°ch h√†ng**, ch√∫ng ta s·∫Ω quay l·∫°i x·ª≠ l√Ω outlier ƒë·ªÉ ƒë·∫£m b·∫£o **t√≠nh ch√≠nh x√°c c·ªßa ph√¢n c·ª•m**.

# ƒê·∫∑t l·∫°i ch·ªâ s·ªë d√≤ng cho t·∫≠p d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch
df.reset_index(drop=True, inplace=True)
# L·∫•y s·ªë l∆∞·ª£ng d√≤ng trong DataFrame
df.shape[0]

Step 4 | Feature Engineering
In order to create a comprehensive customer-centric dataset for clustering and recommendation, the following features can be engineered from the available data:
Step 4.1 | RFM Features
## Gi·ªõi thi·ªáu v·ªÅ RFM

**RFM** l√† m·ªôt ph∆∞∆°ng ph√°p ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ **ph√¢n t√≠ch gi√° tr·ªã kh√°ch h√†ng** v√† **ph√¢n kh√∫c t·∫≠p kh√°ch h√†ng**.  
T√™n g·ªçi **"RFM"** l√† vi·∫øt t·∫Øt c·ªßa ba y·∫øu t·ªë ch√≠nh:

---

### üîπ Recency (R) ‚Äì G·∫ßn ƒë√¢y nh·∫•t:
- Ch·ªâ ra **kh√°ch h√†ng ƒë√£ mua h√†ng g·∫ßn ƒë√¢y ƒë·∫øn m·ª©c n√†o**.
- **Gi√° tr·ªã R c√†ng th·∫•p** ‚Üí kh√°ch h√†ng mua h√†ng c√†ng g·∫ßn ƒë√¢y ‚Üí th·ªÉ hi·ªán **m·ª©c ƒë·ªô t∆∞∆°ng t√°c cao** v·ªõi th∆∞∆°ng hi·ªáu.

---

### üîπ Frequency (F) ‚Äì T·∫ßn su·∫•t mua h√†ng:
- Cho bi·∫øt **kh√°ch h√†ng mua h√†ng bao nhi√™u l·∫ßn** trong m·ªôt kho·∫£ng th·ªùi gian nh·∫•t ƒë·ªãnh.
- **T·∫ßn su·∫•t c√†ng cao** ‚Üí kh√°ch h√†ng c√†ng th∆∞·ªùng xuy√™n quay l·∫°i ‚Üí th·ªÉ hi·ªán **s·ª± trung th√†nh ho·∫∑c m·ª©c ƒë·ªô h√†i l√≤ng cao**.

---

### üîπ Monetary (M) ‚Äì Gi√° tr·ªã ti·ªÅn chi ti√™u:
- L√† **t·ªïng s·ªë ti·ªÅn kh√°ch h√†ng ƒë√£ chi ti√™u** trong m·ªôt kho·∫£ng th·ªùi gian.
- **Gi√° tr·ªã M c√†ng cao** ‚Üí kh√°ch h√†ng ƒë√≥ng g√≥p doanh thu l·ªõn h∆°n ‚Üí ti·ªÅm nƒÉng tr·ªü th√†nh **kh√°ch h√†ng c√≥ gi√° tr·ªã cao** trong d√†i h·∫°n.

---

### ‚úÖ √ù nghƒ©a:
Khi **k·∫øt h·ª£p ba ch·ªâ s·ªë n√†y**, b·∫°n c√≥ th·ªÉ:
- Hi·ªÉu r√µ **h√†nh vi v√† gi√° tr·ªã c·ªßa kh√°ch h√†ng**.
- C√° nh√¢n h√≥a **chi·∫øn l∆∞·ª£c marketing**.
- X√¢y d·ª±ng **h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m** ph√π h·ª£p v√† hi·ªáu qu·∫£ h∆°n.

Step 4.1.1 | Recency (R)
## Ph√¢n t√≠ch M·ª©c ƒê·ªô G·∫ßn ƒê√¢y trong H√†nh Vi Mua H√†ng

Trong b∆∞·ªõc n√†y, ch√∫ng ta s·∫Ω t·∫≠p trung v√†o vi·ªác **hi·ªÉu m·ª©c ƒë·ªô g·∫ßn ƒë√¢y** m√† kh√°ch h√†ng ƒë√£ th·ª±c hi·ªán giao d·ªãch mua h√†ng.  
ƒê√¢y l√† m·ªôt y·∫øu t·ªë quan tr·ªçng trong vi·ªác **ph√¢n kh√∫c kh√°ch h√†ng**, v√¨ n√≥ gi√∫p x√°c ƒë·ªãnh **m·ª©c ƒë·ªô t∆∞∆°ng t√°c c·ªßa kh√°ch h√†ng v·ªõi doanh nghi·ªáp**.

---

### üè∑ Days Since Last Purchase (S·ªë ng√†y k·ªÉ t·ª´ l·∫ßn mua h√†ng g·∫ßn nh·∫•t):
- Th·ªÉ hi·ªán **s·ªë ng√†y ƒë√£ tr√¥i qua** k·ªÉ t·ª´ **l·∫ßn mua h√†ng cu·ªëi c√πng** c·ªßa kh√°ch h√†ng.
- **Gi√° tr·ªã th·∫•p** ‚Üí kh√°ch h√†ng v·ª´a mua h√†ng g·∫ßn ƒë√¢y ‚Üí ph·∫£n √°nh **m·ª©c ƒë·ªô t∆∞∆°ng t√°c cao**.
- **Gi√° tr·ªã cao** ‚Üí d·∫•u hi·ªáu c·ªßa **s·ª± ng·ª´ng t∆∞∆°ng t√°c** ho·∫∑c **m·ª©c ƒë·ªô quan t√¢m gi·∫£m s√∫t**.

---

### ‚úÖ √ù nghƒ©a:
Hi·ªÉu r√µ t√≠nh **"g·∫ßn ƒë√¢y"** trong h√†nh vi mua h√†ng gi√∫p doanh nghi·ªáp:
- **T√πy ch·ªânh chi·∫øn l∆∞·ª£c marketing**, ƒë·∫∑c bi·ªát l√†:
  - **T√°i t∆∞∆°ng t√°c v·ªõi c√°c kh√°ch h√†ng ƒë√£ l√¢u kh√¥ng mua h√†ng**.
- **TƒÉng t·ª∑ l·ªá gi·ªØ ch√¢n kh√°ch h√†ng**.
- **X√¢y d·ª±ng l√≤ng trung th√†nh** b·ªÅn v·ªØng h∆°n.
# Chuy·ªÉn c·ªôt InvoiceDate sang ki·ªÉu d·ªØ li·ªáu datetime
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Tr√≠ch xu·∫•t ph·∫ßn ng√†y (b·ªè gi·ªù) t·ª´ InvoiceDate v√† l∆∞u v√†o c·ªôt m·ªõi InvoiceDay
df['InvoiceDay'] = df['InvoiceDate'].dt.date

# T√¨m ng√†y mua h√†ng g·∫ßn nh·∫•t c·ªßa m·ªói kh√°ch h√†ng
customer_data = df.groupby('CustomerID')['InvoiceDay'].max().reset_index()

# T√¨m ng√†y giao d·ªãch g·∫ßn nh·∫•t trong to√†n b·ªô t·∫≠p d·ªØ li·ªáu
most_recent_date = df['InvoiceDay'].max()

# Chuy·ªÉn c·ªôt InvoiceDay v·ªÅ ki·ªÉu datetime tr∆∞·ªõc khi th·ª±c hi·ªán ph√©p tr·ª´
customer_data['InvoiceDay'] = pd.to_datetime(customer_data['InvoiceDay'])
most_recent_date = pd.to_datetime(most_recent_date)

# T√≠nh s·ªë ng√†y k·ªÉ t·ª´ l·∫ßn mua h√†ng g·∫ßn nh·∫•t c·ªßa m·ªói kh√°ch h√†ng
customer_data['Days_Since_Last_Purchase'] = (most_recent_date - customer_data['InvoiceDay']).dt.days

# Xo√° c·ªôt InvoiceDay v√¨ ƒë√£ t√≠nh xong Recency
customer_data.drop(columns=['InvoiceDay'], inplace=True)

customer_data.head()
T√¥i ƒë√£ ƒë·∫∑t t√™n cho dataframe t·∫≠p trung v√†o kh√°ch h√†ng l√† **`customer_data`**, m√† cu·ªëi c√πng s·∫Ω ch·ª©a t·∫•t c·∫£ c√°c **ƒë·∫∑c ƒëi·ªÉm d·ª±a tr√™n kh√°ch h√†ng** m√† ch√∫ng t√¥i d·ª± ƒë·ªãnh t·∫°o ra.

Step 4.1.2 | Frequency (F)
## T·∫°o ƒê·∫∑c Tr∆∞ng ƒê·ªãnh L∆∞·ª£ng M·ª©c ƒê·ªô T∆∞∆°ng T√°c Kh√°ch H√†ng

Trong b∆∞·ªõc n√†y, t√¥i s·∫Ω t·∫°o ra **hai ƒë·∫∑c tr∆∞ng** nh·∫±m **ƒë·ªãnh l∆∞·ª£ng m·ª©c ƒë·ªô t∆∞∆°ng t√°c c·ªßa kh√°ch h√†ng** v·ªõi nh√† b√°n l·∫ª:

---

### üîπ T·ªïng s·ªë giao d·ªãch (Total Transactions):
- Th·ªÉ hi·ªán **t·ªïng s·ªë l·∫ßn giao d·ªãch** m√† m·ªôt kh√°ch h√†ng ƒë√£ th·ª±c hi·ªán.
- Gi√∫p ƒë√°nh gi√° **m·ª©c ƒë·ªô t∆∞∆°ng t√°c** c·ªßa kh√°ch h√†ng v·ªõi doanh nghi·ªáp.

---

### üîπ T·ªïng s·ªë s·∫£n ph·∫©m ƒë√£ mua (Total Products Purchased):
- Cho bi·∫øt **t·ªïng s·ªë l∆∞·ª£ng s·∫£n ph·∫©m** (t·ªïng **Quantity**) m√† kh√°ch h√†ng ƒë√£ mua qua **t·∫•t c·∫£ c√°c giao d·ªãch**.
- Ph·∫£n √°nh **h√†nh vi mua s·∫Øm** c·ªßa kh√°ch h√†ng v·ªÅ m·∫∑t **kh·ªëi l∆∞·ª£ng s·∫£n ph·∫©m**.

---

### ‚úÖ √ù nghƒ©a:
Hai ƒë·∫∑c tr∆∞ng n√†y s·∫Ω ƒë√≥ng vai tr√≤ **quan tr·ªçng** trong vi·ªác:
- **Ph√¢n kh√∫c kh√°ch h√†ng** d·ª±a tr√™n **t·∫ßn su·∫•t mua h√†ng**.
- L√† **y·∫øu t·ªë then ch·ªët** ƒë·ªÉ x√¢y d·ª±ng:
  - **Chi·∫øn d·ªãch marketing nh·∫Øm m·ª•c ti√™u**.
  - **H·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m c√° nh√¢n h√≥a**.

# T√≠nh t·ªïng s·ªë giao d·ªãch m√† m·ªói kh√°ch h√†ng ƒë√£ th·ª±c hi·ªán
total_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()
total_transactions.rename(columns={'InvoiceNo': 'Total_Transactions'}, inplace=True)

# T√≠nh t·ªïng s·ªë l∆∞·ª£ng s·∫£n ph·∫©m m√† m·ªói kh√°ch h√†ng ƒë√£ mua
total_products_purchased = df.groupby('CustomerID')['Quantity'].sum().reset_index()
total_products_purchased.rename(columns={'Quantity': 'Total_Products_Purchased'}, inplace=True)

# G·ªôp (merge) c√°c ƒë·∫∑c tr∆∞ng m·ªõi v√†o b·∫£ng customer_data theo CustomerID
customer_data = pd.merge(customer_data, total_transactions, on='CustomerID')
customer_data = pd.merge(customer_data, total_products_purchased, on='CustomerID')

# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n c·ªßa b·∫£ng customer_data sau khi c·∫≠p nh·∫≠t
customer_data.head()

Step 4.1.3 | Monetary (M)  
## T·∫°o ƒê·∫∑c Tr∆∞ng Ph·∫£n √Ånh Kh√≠a C·∫°nh Chi Ti√™u (Monetary)

Trong b∆∞·ªõc n√†y, t√¥i s·∫Ω t·∫°o ra **hai ƒë·∫∑c tr∆∞ng** ph·∫£n √°nh **kh√≠a c·∫°nh chi ti√™u** c·ªßa c√°c giao d·ªãch c·ªßa kh√°ch h√†ng:

---

### üîπ T·ªïng chi ti√™u (Total Spend):
- Bi·ªÉu th·ªã **t·ªïng s·ªë ti·ªÅn m√† m·ªói kh√°ch h√†ng ƒë√£ chi ti√™u**.
- ƒê∆∞·ª£c t√≠nh b·∫±ng **t·ªïng c·ªßa (UnitPrice √ó Quantity)** tr√™n **t·∫•t c·∫£ c√°c giao d·ªãch** c·ªßa kh√°ch h√†ng.
- ƒê√¢y l√† m·ªôt ƒë·∫∑c tr∆∞ng **quan tr·ªçng**, v√¨ n√≥ gi√∫p:
  - X√°c ƒë·ªãnh **t·ªïng doanh thu** m√† kh√°ch h√†ng mang l·∫°i.
  - L√† **ch·ªâ b√°o tr·ª±c ti·∫øp** v·ªÅ **gi√° tr·ªã c·ªßa kh√°ch h√†ng** ƒë·ªëi v·ªõi doanh nghi·ªáp.

---

### üîπ Gi√° tr·ªã giao d·ªãch trung b√¨nh (Average Transaction Value):
- ƒê∆∞·ª£c t√≠nh b·∫±ng **T·ªïng chi ti√™u** chia cho **T·ªïng s·ªë giao d·ªãch** c·ªßa m·ªói kh√°ch h√†ng.
- Cho bi·∫øt **gi√° tr·ªã trung b√¨nh c·ªßa m·ªói giao d·ªãch** m√† kh√°ch h√†ng th·ª±c hi·ªán.
- Ch·ªâ s·ªë n√†y r·∫•t h·ªØu √≠ch ƒë·ªÉ:
  - Hi·ªÉu **h√†nh vi chi ti√™u theo t·ª´ng l·∫ßn mua**.
  - H·ªó tr·ª£ x√¢y d·ª±ng c√°c **chi·∫øn l∆∞·ª£c marketing** v√† **∆∞u ƒë√£i ph√π h·ª£p** cho t·ª´ng nh√≥m kh√°ch h√†ng, d·ª±a tr√™n **m√¥ h√¨nh chi ti√™u trung b√¨nh** c·ªßa h·ªç.

---

‚úÖ Hai ƒë·∫∑c tr∆∞ng n√†y s·∫Ω gi√∫p **ph√¢n lo·∫°i kh√°ch h√†ng** d·ª±a tr√™n **gi√° tr·ªã t√†i ch√≠nh**, t·ª´ ƒë√≥ x√¢y d·ª±ng **chi·∫øn l∆∞·ª£c kinh doanh** v√† **h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m** ch√≠nh x√°c h∆°n.
# T√≠nh t·ªïng chi ti√™u c·ªßa m·ªói kh√°ch h√†ng
df['Total_Spend'] = df['UnitPrice'] * df['Quantity']
total_spend = df.groupby('CustomerID')['Total_Spend'].sum().reset_index()

# T√≠nh gi√° tr·ªã giao d·ªãch trung b√¨nh c·ªßa m·ªói kh√°ch h√†ng
average_transaction_value = total_spend.merge(total_transactions, on='CustomerID')
average_transaction_value['Average_Transaction_Value'] = average_transaction_value['Total_Spend'] / average_transaction_value['Total_Transactions']

# G·ªôp c√°c ƒë·∫∑c tr∆∞ng m·ªõi v√†o b·∫£ng customer_data theo CustomerID
customer_data = pd.merge(customer_data, total_spend, on='CustomerID')
customer_data = pd.merge(customer_data, average_transaction_value[['CustomerID', 'Average_Transaction_Value']], on='CustomerID')

# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n c·ªßa b·∫£ng customer_data sau khi c·∫≠p nh·∫≠t
customer_data.head()

#Gi·∫£i th√≠ch l√Ω do t·∫°i sao customerID 12346 c√≥ 2 transaction
df[df['CustomerID'] == 12346.0][['InvoiceNo', 'UnitPrice', 'Quantity', 'Total_Spend']]
## Ph√¢n t√≠ch S·ª± ƒêa D·∫°ng trong H√†nh Vi Mua S·∫£n Ph·∫©m

Trong b∆∞·ªõc n√†y, ch√∫ng ta s·∫Ω **t√¨m hi·ªÉu s·ª± ƒëa d·∫°ng** trong h√†nh vi mua s·∫£n ph·∫©m c·ªßa kh√°ch h√†ng.  
Vi·ªác hi·ªÉu ƒë∆∞·ª£c m·ª©c ƒë·ªô **ƒëa d·∫°ng s·∫£n ph·∫©m m√† kh√°ch h√†ng mua** c√≥ th·ªÉ gi√∫p doanh nghi·ªáp:
- Thi·∫øt k·∫ø c√°c **chi·∫øn l∆∞·ª£c marketing c√° nh√¢n h√≥a**.
- ƒê·ªÅ xu·∫•t s·∫£n ph·∫©m **ph√π h·ª£p h∆°n** v·ªõi t·ª´ng nh√≥m kh√°ch h√†ng.

---

### üîπ S·ªë l∆∞·ª£ng s·∫£n ph·∫©m kh√°c nhau ƒë√£ mua (Unique Products Purchased):
- Bi·ªÉu th·ªã **s·ªë l∆∞·ª£ng s·∫£n ph·∫©m kh√°c nhau** m√† m·ªôt kh√°ch h√†ng ƒë√£ mua.
- **Gi√° tr·ªã cao** ‚Üí kh√°ch h√†ng c√≥ **th·ªã hi·∫øu ƒëa d·∫°ng**, mua nhi·ªÅu lo·∫°i s·∫£n ph·∫©m kh√°c nhau.
- **Gi√° tr·ªã th·∫•p** ‚Üí kh√°ch h√†ng c√≥ **s·ªü th√≠ch c·ª• th·ªÉ**, th∆∞·ªùng mua t·∫≠p trung v√†o m·ªôt v√†i m·∫∑t h√†ng nh·∫•t ƒë·ªãnh.

---

### ‚úÖ √ù nghƒ©a:
Vi·ªác ph√¢n t√≠ch m·ª©c ƒë·ªô **ƒëa d·∫°ng trong h√†nh vi mua h√†ng** s·∫Ω gi√∫p:
- Ph√¢n nh√≥m kh√°ch h√†ng d·ª±a tr√™n **ƒë·ªô phong ph√∫ trong l·ª±a ch·ªçn s·∫£n ph·∫©m**.
- L√† **y·∫øu t·ªë quan tr·ªçng** trong vi·ªác x√¢y d·ª±ng **g·ª£i √Ω s·∫£n ph·∫©m c√° nh√¢n h√≥a** v√† chi·∫øn l∆∞·ª£c chƒÉm s√≥c kh√°ch h√†ng ph√π h·ª£p.

---


# T√≠nh s·ªë l∆∞·ª£ng s·∫£n ph·∫©m kh√°c nhau m√† m·ªói kh√°ch h√†ng ƒë√£ mua
unique_products_purchased = df.groupby('CustomerID')['StockCode'].nunique().reset_index()
unique_products_purchased.rename(columns={'StockCode': 'Unique_Products_Purchased'}, inplace=True)

# G·ªôp ƒë·∫∑c tr∆∞ng m·ªõi v√†o b·∫£ng customer_data theo CustomerID
customer_data = pd.merge(customer_data, unique_products_purchased, on='CustomerID')

# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n c·ªßa b·∫£ng customer_data sau khi c·∫≠p nh·∫≠t
customer_data.head()

Step 4.3 | Behavioral Features
## Ph√¢n t√≠ch M√¥ h√¨nh Mua S·∫Øm v√† H√†nh Vi Kh√°ch H√†ng

üéØ Trong b∆∞·ªõc n√†y, ch√∫ng ta s·∫Ω **t√¨m hi·ªÉu v√† ghi nh·∫≠n c√°c m√¥ h√¨nh mua s·∫Øm (shopping patterns)** c≈©ng nh∆∞ **h√†nh vi c·ªßa kh√°ch h√†ng**.

Nh·ªØng ƒë·∫∑c tr∆∞ng n√†y s·∫Ω cung c·∫•p **c√°i nh√¨n s√¢u h∆°n v·ªÅ th√≥i quen v√† s·ªü th√≠ch mua s·∫Øm**, t·ª´ ƒë√≥ gi√∫p **c√° nh√¢n h√≥a tr·∫£i nghi·ªám mua s·∫Øm** m·ªôt c√°ch hi·ªáu qu·∫£ h∆°n.

---

### üîπ S·ªë ng√†y trung b√¨nh gi·ªØa c√°c l·∫ßn mua (Average Days Between Purchases):
- Ph·∫£n √°nh **s·ªë ng√†y trung b√¨nh m√† kh√°ch h√†ng ch·ªù gi·ªØa c√°c l·∫ßn mua h√†ng**.
- Hi·ªÉu ƒë∆∞·ª£c kho·∫£ng c√°ch gi·ªØa c√°c giao d·ªãch gi√∫p:
  - **D·ª± ƒëo√°n th·ªùi ƒëi·ªÉm mua ti·∫øp theo**.
  - H·ªó tr·ª£ hi·ªáu qu·∫£ cho **c√°c chi·∫øn d·ªãch marketing nh·∫Øm m·ª•c ti√™u** v√† **khuy·∫øn m√£i c√° nh√¢n h√≥a**.

---

### üîπ Ng√†y mua s·∫Øm ∆∞a th√≠ch (Favorite Shopping Day):
- L√† **ng√†y trong tu·∫ßn** m√† kh√°ch h√†ng **mua h√†ng nhi·ªÅu nh·∫•t**.
- Th√¥ng tin n√†y gi√∫p:
  - X√°c ƒë·ªãnh ng√†y kh√°ch h√†ng th∆∞·ªùng mua s·∫Øm.
  - **T·ªëi ∆∞u h√≥a c√°c chi·∫øn d·ªãch marketing** v√† ch∆∞∆°ng tr√¨nh khuy·∫øn m√£i theo **t·ª´ng ng√†y trong tu·∫ßn**.

---

### üîπ Gi·ªù mua s·∫Øm ∆∞a th√≠ch (Favorite Shopping Hour):
- L√† **gi·ªù trong ng√†y** m√† kh√°ch h√†ng **mua h√†ng nhi·ªÅu nh·∫•t**.
- Vi·ªác x√°c ƒë·ªãnh khung gi·ªù mua s·∫Øm ∆∞a th√≠ch gi√∫p doanh nghi·ªáp:
  - ƒêi·ªÅu ch·ªânh th·ªùi ƒëi·ªÉm ch·∫°y **qu·∫£ng c√°o**, **email marketing**, **∆∞u ƒë√£i**...
  - Nh·∫Øm tr√∫ng **th·ªùi ƒëi·ªÉm kh√°ch h√†ng c√≥ kh·∫£ nƒÉng t∆∞∆°ng t√°c cao nh·∫•t**.

---

### ‚úÖ √ù nghƒ©a:
Vi·ªác b·ªï sung c√°c **ƒë·∫∑c tr∆∞ng h√†nh vi** n√†y v√†o t·∫≠p d·ªØ li·ªáu s·∫Ω gi√∫p:
- Hi·ªÉu kh√°ch h√†ng **to√†n di·ªán h∆°n**.
- N√¢ng cao hi·ªáu qu·∫£ c·ªßa thu·∫≠t to√°n **ph√¢n c·ª•m kh√°ch h√†ng (clustering)**.
- T·∫°o ra nh·ªØng **nh√≥m kh√°ch h√†ng c√≥ √Ω nghƒ©a v√† h√†nh ƒë·ªông r√µ r√†ng h∆°n** ƒë·ªÉ ph·ª•c v·ª• m·ª•c ti√™u **marketing v√† g·ª£i √Ω s·∫£n ph·∫©m**.

---

# Tr√≠ch xu·∫•t ng√†y trong tu·∫ßn v√† gi·ªù trong ng√†y t·ª´ c·ªôt InvoiceDate
df['Day_Of_Week'] = df['InvoiceDate'].dt.dayofweek
df['Hour'] = df['InvoiceDate'].dt.hour

# T√≠nh s·ªë ng√†y trung b√¨nh gi·ªØa c√°c l·∫ßn mua h√†ng li√™n ti·∫øp c·ªßa m·ªói kh√°ch h√†ng
days_between_purchases = df.groupby('CustomerID')['InvoiceDay'].apply(lambda x: (x.diff().dropna()).apply(lambda y: y.days))
average_days_between_purchases = days_between_purchases.groupby('CustomerID').mean().reset_index()
average_days_between_purchases.rename(columns={'InvoiceDay': 'Average_Days_Between_Purchases'}, inplace=True)

# T√¨m ng√†y trong tu·∫ßn m√† m·ªói kh√°ch h√†ng mua h√†ng nhi·ªÅu nh·∫•t (ng√†y mua s·∫Øm ∆∞a th√≠ch)
favorite_shopping_day = df.groupby(['CustomerID', 'Day_Of_Week']).size().reset_index(name='Count')
favorite_shopping_day = favorite_shopping_day.loc[favorite_shopping_day.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Day_Of_Week']]

# T√¨m gi·ªù trong ng√†y m√† m·ªói kh√°ch h√†ng mua h√†ng nhi·ªÅu nh·∫•t (gi·ªù mua s·∫Øm ∆∞a th√≠ch)
favorite_shopping_hour = df.groupby(['CustomerID', 'Hour']).size().reset_index(name='Count')
favorite_shopping_hour = favorite_shopping_hour.loc[favorite_shopping_hour.groupby('CustomerID')['Count'].idxmax()][['CustomerID', 'Hour']]

# G·ªôp c√°c ƒë·∫∑c tr∆∞ng h√†nh vi m·ªõi v√†o b·∫£ng customer_data theo CustomerID
customer_data = pd.merge(customer_data, average_days_between_purchases, on='CustomerID')
customer_data = pd.merge(customer_data, favorite_shopping_day, on='CustomerID')
customer_data = pd.merge(customer_data, favorite_shopping_hour, on='CustomerID')

# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n c·ªßa b·∫£ng customer_data sau khi c·∫≠p nh·∫≠t
customer_data.head()

Step 4.4 | Geographic Features
## Th√™m ƒê·∫∑c Tr∆∞ng V·ªã Tr√≠ ƒê·ªãa L√Ω Kh√°ch H√†ng

Trong b∆∞·ªõc n√†y, ch√∫ng ta s·∫Ω th√™m m·ªôt **ƒë·∫∑c tr∆∞ng v·ªÅ v·ªã tr√≠ ƒë·ªãa l√Ω** ph·∫£n √°nh **khu v·ª±c sinh s·ªëng c·ªßa kh√°ch h√†ng**.

Vi·ªác hi·ªÉu r√µ **ph√¢n b·ªë ƒë·ªãa l√Ω c·ªßa kh√°ch h√†ng** l√† r·∫•t quan tr·ªçng v√¨ nhi·ªÅu l√Ω do sau:

---

### üîπ Qu·ªëc gia (Country):
- ƒê·∫∑c tr∆∞ng n√†y cho bi·∫øt **kh√°ch h√†ng ƒë·∫øn t·ª´ qu·ªëc gia n√†o**.
- Vi·ªác ƒë∆∞a d·ªØ li·ªáu qu·ªëc gia v√†o s·∫Ω gi√∫p:
  - Hi·ªÉu ƒë∆∞·ª£c **c√°c xu h∆∞·ªõng mua s·∫Øm v√† s·ªü th√≠ch theo khu v·ª±c**.
  - Ph√°t hi·ªán r·∫±ng **m·ªói khu v·ª±c c√≥ th·ªÉ c√≥ s·ªü th√≠ch, h√†nh vi mua h√†ng kh√°c nhau** ‚Üí ·∫£nh h∆∞·ªüng ƒë·∫øn **chi·∫øn l∆∞·ª£c marketing c√° nh√¢n h√≥a**.

---

### ‚úÖ L·ª£i √≠ch:
- **H·ªó tr·ª£ l·∫≠p k·∫ø ho·∫°ch t·ªìn kho** ch√≠nh x√°c h∆°n, ph√π h·ª£p v·ªõi nhu c·∫ßu t·∫°i t·ª´ng v√πng.
- ƒê√≥ng vai tr√≤ quan tr·ªçng trong **t·ªëi ∆∞u h√≥a logistics v√† chu·ªói cung ·ª©ng**, ƒë·∫∑c bi·ªát v·ªõi c√°c nh√† b√°n l·∫ª tr·ª±c tuy·∫øn, n∆°i **v·∫≠n chuy·ªÉn v√† giao h√†ng** l√† y·∫øu t·ªë then ch·ªët.

---

df['Country'].value_counts(normalize=True).head()
## Nh·∫≠n ƒë·ªãnh:

- **V√¨ m·ªôt t·ª∑ l·ªá l·ªõn (89%) c√°c giao d·ªãch ƒë·∫øn t·ª´ V∆∞∆°ng qu·ªëc Anh (United Kingdom)**, n√™n ch√∫ng ta c√≥ th·ªÉ c√¢n nh·∫Øc **t·∫°o m·ªôt bi·∫øn nh·ªã ph√¢n** ƒë·ªÉ bi·ªÉu th·ªã li·ªáu **m·ªôt giao d·ªãch c√≥ ƒë·∫øn t·ª´ UK hay kh√¥ng**.

---

### L·ª£i √≠ch c·ªßa c√°ch ti·∫øp c·∫≠n n√†y:
- Gi√∫p **ƒë∆°n gi·∫£n h√≥a qu√° tr√¨nh ph√¢n c·ª•m** (clustering).
- V·∫´n **gi·ªØ l·∫°i th√¥ng tin ƒë·ªãa l√Ω quan tr·ªçng**.
- ƒê·∫∑c bi·ªát h·ªØu √≠ch khi √°p d·ª•ng c√°c thu·∫≠t to√°n nh∆∞ **K-means**, v·ªën **nh·∫°y c·∫£m v·ªõi s·ªë chi·ªÅu (dimensionality)** c·ªßa kh√¥ng gian ƒë·∫∑c tr∆∞ng.

---

‚úÖ Bi·∫øn nh·ªã ph√¢n `Is_UK_Customer` c√≥ th·ªÉ ƒë∆∞·ª£c t·∫°o, v·ªõi gi√° tr·ªã:
- **1** n·∫øu kh√°ch h√†ng ƒë·∫øn t·ª´ UK.
- **0** n·∫øu kh√°ch h√†ng ƒë·∫øn t·ª´ qu·ªëc gia kh√°c.

---

## Ph∆∞∆°ng ph√°p th·ª±c hi·ªán:

- **ƒê·∫ßu ti√™n**, t√¥i s·∫Ω **group d·ªØ li·ªáu theo CustomerID v√† Country**, sau ƒë√≥ **t√≠nh s·ªë l∆∞·ª£ng giao d·ªãch** t·∫°i m·ªói qu·ªëc gia ƒë·ªëi v·ªõi t·ª´ng kh√°ch h√†ng.

- **Ti·∫øp theo**, t√¥i s·∫Ω **x√°c ƒë·ªãnh qu·ªëc gia ch√≠nh c·ªßa m·ªói kh√°ch h√†ng** ‚Äî t·ª©c l√† **qu·ªëc gia m√† h·ªç th·ª±c hi·ªán nhi·ªÅu giao d·ªãch nh·∫•t**.

- Sau ƒë√≥, t√¥i s·∫Ω **t·∫°o m·ªôt c·ªôt nh·ªã ph√¢n (binary column)** ƒë·ªÉ x√°c ƒë·ªãnh kh√°ch h√†ng c√≥ ƒë·∫øn t·ª´ **V∆∞∆°ng qu·ªëc Anh (UK)** hay kh√¥ng.

- **Cu·ªëi c√πng**, t√¥i s·∫Ω **g·ªôp (merge)** th√¥ng tin n√†y v√†o b·∫£ng **customer_data** ƒë·ªÉ **b·ªï sung ƒë·∫∑c tr∆∞ng ƒë·ªãa l√Ω m·ªõi** cho qu√° tr√¨nh ph√¢n t√≠ch.
# Nh√≥m d·ªØ li·ªáu theo CustomerID v√† Country ƒë·ªÉ t√≠nh s·ªë giao d·ªãch t·∫°i m·ªói qu·ªëc gia cho t·ª´ng kh√°ch h√†ng
customer_country = df.groupby(['CustomerID', 'Country']).size().reset_index(name='Number_of_Transactions')

# L·∫•y qu·ªëc gia c√≥ s·ªë l∆∞·ª£ng giao d·ªãch nhi·ªÅu nh·∫•t cho m·ªói kh√°ch h√†ng (tr∆∞·ªùng h·ª£p kh√°ch h√†ng c√≥ nhi·ªÅu qu·ªëc gia kh√°c nhau)
customer_main_country = customer_country.sort_values('Number_of_Transactions', ascending=False).drop_duplicates('CustomerID')

# T·∫°o c·ªôt nh·ªã ph√¢n ƒë·ªÉ x√°c ƒë·ªãnh kh√°ch h√†ng c√≥ ƒë·∫øn t·ª´ UK hay kh√¥ng (1 n·∫øu l√† UK, ng∆∞·ª£c l·∫°i l√† 0)
customer_main_country['Is_UK'] = customer_main_country['Country'].apply(lambda x: 1 if x == 'United Kingdom' else 0)

# G·ªôp th√¥ng tin qu·ªëc gia ch√≠nh c·ªßa kh√°ch h√†ng v√†o b·∫£ng customer_data
customer_data = pd.merge(customer_data, customer_main_country[['CustomerID', 'Is_UK']], on='CustomerID', how='left')

# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n c·ªßa b·∫£ng customer_data sau khi c·∫≠p nh·∫≠t
customer_data.head()

customer_data['Is_UK'].value_counts()

Step 4.5 | Cancellation Insights
## Ph√¢n t√≠ch M·∫´u H√†nh Vi H·ªßy ƒê∆°n H√†ng c·ªßa Kh√°ch H√†ng

Trong b∆∞·ªõc n√†y, t√¥i s·∫Ω **ƒëi s√¢u v√†o ph√¢n t√≠ch c√°c m·∫´u h√†nh vi h·ªßy ƒë∆°n h√†ng c·ªßa kh√°ch h√†ng** nh·∫±m thu th·∫≠p nh·ªØng th√¥ng tin c√≥ th·ªÉ n√¢ng cao hi·ªáu qu·∫£ m√¥ h√¨nh ph√¢n kh√∫c kh√°ch h√†ng.

---

### üîπ T·∫ßn su·∫•t h·ªßy ƒë∆°n (Cancellation Frequency):
- Ch·ªâ s·ªë n√†y th·ªÉ hi·ªán **t·ªïng s·ªë giao d·ªãch m√† kh√°ch h√†ng ƒë√£ h·ªßy**.
- Vi·ªác hi·ªÉu r√µ t·∫ßn su·∫•t h·ªßy ƒë∆°n gi√∫p:
  - X√°c ƒë·ªãnh nh·ªØng kh√°ch h√†ng **c√≥ xu h∆∞·ªõng hay h·ªßy ƒë∆°n**.
  - Ph√°t hi·ªán c√°c **v·∫•n ƒë·ªÅ ti·ªÅm ·∫©n** ho·∫∑c d·∫•u hi·ªáu c·ªßa **s·ª± kh√¥ng h√†i l√≤ng**.
- Vi·ªác ph√°t hi·ªán s·ªõm h√†nh vi n√†y gi√∫p doanh nghi·ªáp:
  - ƒêi·ªÅu ch·ªânh chi·∫øn l∆∞·ª£c ƒë·ªÉ **gi·∫£m thi·ªÉu t·ª∑ l·ªá h·ªßy ƒë∆°n**.
  - **TƒÉng s·ª± h√†i l√≤ng c·ªßa kh√°ch h√†ng**.

---

### üîπ T·ª∑ l·ªá h·ªßy ƒë∆°n (Cancellation Rate):
- L√† **t·ª∑ l·ªá gi·ªØa s·ªë l∆∞·ª£ng ƒë∆°n b·ªã h·ªßy v√† t·ªïng s·ªë giao d·ªãch c·ªßa kh√°ch h√†ng**.
- Ch·ªâ s·ªë n√†y cung c·∫•p m·ªôt c√°i nh√¨n **chu·∫©n h√≥a v·ªÅ h√†nh vi h·ªßy ƒë∆°n**.
- **T·ª∑ l·ªá h·ªßy cao** c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa:
  - M·ªôt ph√¢n kh√∫c kh√°ch h√†ng **kh√¥ng h√†i l√≤ng**.
  - Doanh nghi·ªáp c·∫ßn x√¢y d·ª±ng **chi·∫øn l∆∞·ª£c nh·∫Øm m·ª•c ti√™u c·ª• th·ªÉ** ƒë·ªÉ:
    - C·∫£i thi·ªán tr·∫£i nghi·ªám mua s·∫Øm.
    - Gi·∫£m t·ª∑ l·ªá h·ªßy.

---

### ‚úÖ √ù nghƒ©a:
Vi·ªác t√≠ch h·ª£p c√°c th√¥ng tin v·ªÅ **h√†nh vi h·ªßy ƒë∆°n** v√†o t·∫≠p d·ªØ li·ªáu s·∫Ω gi√∫p:
- X√¢y d·ª±ng m·ªôt **b·ª©c tranh to√†n di·ªán h∆°n v·ªÅ h√†nh vi kh√°ch h√†ng**.
- H·ªó tr·ª£ **ph√¢n kh√∫c kh√°ch h√†ng ch√≠nh x√°c v√† hi·ªáu qu·∫£ h∆°n**.
# T√≠nh t·ªïng s·ªë giao d·ªãch c·ªßa m·ªói kh√°ch h√†ng
total_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()

# L·ªçc c√°c giao d·ªãch b·ªã h·ªßy (Transaction_Status = 'Cancelled') v√† t√≠nh s·ªë l∆∞·ª£ng giao d·ªãch b·ªã h·ªßy c·ªßa m·ªói kh√°ch h√†ng
cancelled_transactions = df[df['Transaction_Status'] == 'Cancelled']
cancellation_frequency = cancelled_transactions.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()
cancellation_frequency.rename(columns={'InvoiceNo': 'Cancellation_Frequency'}, inplace=True)

# G·ªôp s·ªë l∆∞·ª£ng h·ªßy ƒë∆°n v√†o b·∫£ng customer_data theo CustomerID
customer_data = pd.merge(customer_data, cancellation_frequency, on='CustomerID', how='left')

# Thay th·∫ø gi√° tr·ªã NaN b·∫±ng 0 (v·ªõi nh·ªØng kh√°ch h√†ng kh√¥ng h·ªßy ƒë∆°n n√†o)
customer_data['Cancellation_Frequency'].fillna(0, inplace=True)

# T√≠nh t·ª∑ l·ªá h·ªßy ƒë∆°n = s·ªë ƒë∆°n b·ªã h·ªßy / t·ªïng s·ªë giao d·ªãch
customer_data['Cancellation_Rate'] = customer_data['Cancellation_Frequency'] / total_transactions['InvoiceNo']

# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n c·ªßa b·∫£ng customer_data sau khi c·∫≠p nh·∫≠t
customer_data.head()

Step 4.6 | Seasonality & Trends
## Ph√¢n t√≠ch Y·∫øu T·ªë M√πa V·ª• v√† Xu H∆∞·ªõng Chi Ti√™u C·ªßa Kh√°ch H√†ng

Trong b∆∞·ªõc n√†y, t√¥i s·∫Ω ph√¢n t√≠ch s√¢u h∆°n v·ªÅ y·∫øu t·ªë **m√πa v·ª• (seasonality)** v√† **xu h∆∞·ªõng (trend)** trong h√†nh vi chi ti√™u c·ªßa kh√°ch h√†ng.  
ƒêi·ªÅu n√†y cung c·∫•p nh·ªØng **hi·ªÉu bi·∫øt v√¥ c√πng gi√° tr·ªã** ƒë·ªÉ t√πy ch·ªânh **chi·∫øn l∆∞·ª£c marketing** v√† n√¢ng cao **s·ª± h√†i l√≤ng c·ªßa kh√°ch h√†ng**.

---

### C√°c ƒë·∫∑c tr∆∞ng d·ª± ki·∫øn s·∫Ω th√™m v√†o:

---

### üîπ Monthly_Spending_Mean (Chi ti√™u trung b√¨nh theo th√°ng):
- L√† **gi√° tr·ªã chi ti√™u trung b√¨nh h√†ng th√°ng** c·ªßa m·ªói kh√°ch h√†ng.
- Gi√∫p ƒë√°nh gi√° **th√≥i quen chi ti√™u t·ªïng th·ªÉ** c·ªßa kh√°ch.
- √ù nghƒ©a:
  - **Gi√° tr·ªã trung b√¨nh cao** ‚Üí kh√°ch h√†ng chi ti√™u nhi·ªÅu, c√≥ th·ªÉ quan t√¢m ƒë·∫øn **s·∫£n ph·∫©m cao c·∫•p**.
  - **Gi√° tr·ªã trung b√¨nh th·∫•p** ‚Üí kh√°ch h√†ng c√≥ xu h∆∞·ªõng **chi ti√™u ti·∫øt ki·ªám** h∆°n.

---

### üîπ Monthly_Spending_Std (ƒê·ªô l·ªách chu·∫©n chi ti√™u h√†ng th√°ng):
- Ph·∫£n √°nh **m·ª©c ƒë·ªô dao ƒë·ªông trong chi ti√™u h√†ng th√°ng** c·ªßa kh√°ch h√†ng.
- √ù nghƒ©a:
  - **ƒê·ªô l·ªách chu·∫©n cao** ‚Üí chi ti√™u **kh√¥ng ·ªïn ƒë·ªãnh**, c√≥ th·ªÉ c√≥ c√°c **giao d·ªãch l·ªõn b·∫•t th∆∞·ªùng**.
  - **ƒê·ªô l·ªách chu·∫©n th·∫•p** ‚Üí kh√°ch h√†ng c√≥ **th√≥i quen chi ti√™u ƒë·ªÅu ƒë·∫∑n, ·ªïn ƒë·ªãnh**.
- Hi·ªÉu ƒë∆∞·ª£c ƒë·ªô bi·∫øn ƒë·ªông n√†y gi√∫p:
  - **Thi·∫øt k·∫ø khuy·∫øn m√£i c√° nh√¢n h√≥a**, ƒë·∫∑c bi·ªát v√†o c√°c th·ªùi ƒëi·ªÉm kh√°ch c√≥ xu h∆∞·ªõng chi ti√™u cao h∆°n.

---

### üîπ Spending_Trend (Xu h∆∞·ªõng chi ti√™u):
- Ph·∫£n √°nh **xu h∆∞·ªõng thay ƒë·ªïi chi ti√™u theo th·ªùi gian** c·ªßa m·ªói kh√°ch h√†ng.
- ƒê∆∞·ª£c t√≠nh b·∫±ng **ƒë·ªô d·ªëc c·ªßa ƒë∆∞·ªùng h·ªìi quy tuy·∫øn t√≠nh** d·ª±a tr√™n d·ªØ li·ªáu chi ti√™u theo th·ªùi gian.
- √ù nghƒ©a:
  - **Gi√° tr·ªã d∆∞∆°ng** ‚Üí xu h∆∞·ªõng **chi ti√™u tƒÉng d·∫ßn**, c√≥ th·ªÉ l√† d·∫•u hi·ªáu c·ªßa **s·ª± trung th√†nh** ho·∫∑c **m·ª©c ƒë·ªô h√†i l√≤ng tƒÉng**.
  - **Gi√° tr·ªã √¢m** ‚Üí xu h∆∞·ªõng **gi·∫£m chi ti√™u**, c√≥ th·ªÉ b√°o hi·ªáu **s·ª± gi·∫£m quan t√¢m** ho·∫∑c **kh√¥ng h√†i l√≤ng**.
  - **G·∫ßn b·∫±ng 0** ‚Üí **th√≥i quen chi ti√™u ·ªïn ƒë·ªãnh**.
- Nh·∫≠n di·ªán xu h∆∞·ªõng n√†y gi√∫p doanh nghi·ªáp:
  - **Duy tr√¨** ho·∫∑c **ƒëi·ªÅu ch·ªânh** h√†nh vi chi ti√™u.
  - **TƒÉng hi·ªáu qu·∫£ c√°c chi·∫øn d·ªãch marketing**.

---

### ‚úÖ √ù nghƒ©a t·ªïng quan:
Vi·ªác t√≠ch h·ª£p c√°c ƒë·∫∑c tr∆∞ng chi ti·∫øt n√†y v√†o **m√¥ h√¨nh ph√¢n kh√∫c kh√°ch h√†ng** gi√∫p:
- X√¢y d·ª±ng **nh√≥m kh√°ch h√†ng ch√≠nh x√°c v√† h√†nh ƒë·ªông ƒë∆∞·ª£c h∆°n**.
- Thi·∫øt k·∫ø c√°c **chi·∫øn l∆∞·ª£c marketing** v√† **khuy·∫øn m√£i** ƒë∆∞·ª£c **c√° nh√¢n h√≥a t·ªëi ƒëa**.

# Tr√≠ch xu·∫•t nƒÉm v√† th√°ng t·ª´ c·ªôt InvoiceDate
df['Year'] = df['InvoiceDate'].dt.year
df['Month'] = df['InvoiceDate'].dt.month

# T√≠nh t·ªïng chi ti√™u h√†ng th√°ng c·ªßa m·ªói kh√°ch h√†ng
monthly_spending = df.groupby(['CustomerID', 'Year', 'Month'])['Total_Spend'].sum().reset_index()

# T√≠nh ƒë·∫∑c tr∆∞ng mua s·∫Øm theo m√πa: s·ª≠ d·ª•ng chi ti√™u h√†ng th√°ng ƒë·ªÉ bi·ªÉu di·ªÖn h√†nh vi m√πa v·ª•
seasonal_buying_patterns = monthly_spending.groupby('CustomerID')['Total_Spend'].agg(['mean', 'std']).reset_index()
seasonal_buying_patterns.rename(columns={'mean': 'Monthly_Spending_Mean', 'std': 'Monthly_Spending_Std'}, inplace=True)

# Thay gi√° tr·ªã NaN trong Monthly_Spending_Std b·∫±ng 0 (√°p d·ª•ng cho kh√°ch ch·ªâ mua trong 1 th√°ng)
seasonal_buying_patterns['Monthly_Spending_Std'].fillna(0, inplace=True)

# T√≠nh xu h∆∞·ªõng chi ti√™u theo th·ªùi gian
# S·ª≠ d·ª•ng ƒë·ªô d·ªëc (slope) c·ªßa ƒë∆∞·ªùng h·ªìi quy tuy·∫øn t√≠nh ƒë·ªÉ bi·ªÉu di·ªÖn xu h∆∞·ªõng chi ti√™u c·ªßa kh√°ch

def calculate_trend(spend_data):
    # N·∫øu c√≥ nhi·ªÅu h∆°n 1 ƒëi·ªÉm d·ªØ li·ªáu, t√≠nh ƒë·ªô d·ªëc c·ªßa ƒë∆∞·ªùng h·ªìi quy tuy·∫øn t√≠nh
    if len(spend_data) > 1:
        x = np.arange(len(spend_data))
        slope, _, _, _, _ = linregress(x, spend_data)
        return slope
    # N·∫øu ch·ªâ c√≥ 1 ƒëi·ªÉm d·ªØ li·ªáu th√¨ kh√¥ng th·ªÉ t√≠nh ƒë∆∞·ª£c xu h∆∞·ªõng, tr·∫£ v·ªÅ 0
    else:
        return 0

# √Åp d·ª•ng h√†m calculate_trend ƒë·ªÉ t√≠nh xu h∆∞·ªõng chi ti√™u cho t·ª´ng kh√°ch h√†ng
spending_trends = monthly_spending.groupby('CustomerID')['Total_Spend'].apply(calculate_trend).reset_index()
spending_trends.rename(columns={'Total_Spend': 'Spending_Trend'}, inplace=True)

# G·ªôp c√°c ƒë·∫∑c tr∆∞ng m·ªõi v√†o b·∫£ng customer_data
customer_data = pd.merge(customer_data, seasonal_buying_patterns, on='CustomerID')
customer_data = pd.merge(customer_data, spending_trends, on='CustomerID')

# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n c·ªßa b·∫£ng customer_data sau khi c·∫≠p nh·∫≠t
customer_data.head()

We've done a great job so far! We have created a dataset that focuses on our customers, using a variety of new features that give us a deeper understanding of their buying patterns and preferences.
*** Ti·∫øng Vi·ªát ***
Ch√∫ng ta ƒë√£ ho√†n th√†nh m·ªôt c√¥ng vi·ªác tuy·ªát v·ªùi cho ƒë·∫øn th·ªùi ƒëi·ªÉm n√†y!
Ch√∫ng ta ƒë√£ t·∫°o ra m·ªôt t·∫≠p d·ªØ li·ªáu t·∫≠p trung v√†o kh√°ch h√†ng, s·ª≠ d·ª•ng nhi·ªÅu ƒë·∫∑c tr∆∞ng m·ªõi gi√∫p hi·ªÉu s√¢u h∆°n v·ªÅ h√†nh vi mua s·∫Øm v√† s·ªü th√≠ch c·ªßa h·ªç.
# Chuy·ªÉn ki·ªÉu d·ªØ li·ªáu c·ªßa 'CustomerID' th√†nh chu·ªói (string) v√¨ ƒë√¢y l√† m√£ ƒë·ªãnh danh, kh√¥ng d√πng ƒë·ªÉ t√≠nh to√°n
customer_data['CustomerID'] = customer_data['CustomerID'].astype(str)

# T·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi ki·ªÉu d·ªØ li·ªáu c√°c c·ªôt sang ki·ªÉu t·ªëi ∆∞u nh·∫•t (v√≠ d·ª•: int -> Int64, float -> Float64, object -> string,...)
customer_data = customer_data.convert_dtypes()

customer_data.head(10)
customer_data.info()
## M√¥ T·∫£ Dataset Kh√°ch H√†ng (Customer Dataset Description)

| Bi·∫øn                        | M√¥ T·∫£                                                                                                                                                                |
|----------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| CustomerID                 | M√£ ƒë·ªãnh danh duy nh·∫•t ƒë∆∞·ª£c g√°n cho t·ª´ng kh√°ch h√†ng, d√πng ƒë·ªÉ ph√¢n bi·ªát c√°c kh√°ch h√†ng.                                                                              |
| Days_Since_Last_Purchase   | S·ªë ng√†y ƒë√£ tr√¥i qua k·ªÉ t·ª´ l·∫ßn mua h√†ng g·∫ßn nh·∫•t c·ªßa kh√°ch h√†ng.                                                                                                    |
| Total_Transactions         | T·ªïng s·ªë giao d·ªãch m√† kh√°ch h√†ng ƒë√£ th·ª±c hi·ªán.                                                                                                                       |
| Total_Products_Purchased   | T·ªïng s·ªë l∆∞·ª£ng s·∫£n ph·∫©m m√† kh√°ch h√†ng ƒë√£ mua qua t·∫•t c·∫£ c√°c giao d·ªãch.                                                                                                |
| Total_Spend                | T·ªïng s·ªë ti·ªÅn m√† kh√°ch h√†ng ƒë√£ chi ti√™u qua t·∫•t c·∫£ c√°c giao d·ªãch.                                                                                                     |
| Average_Transaction_Value  | Gi√° tr·ªã trung b√¨nh c·ªßa c√°c giao d·ªãch, ƒë∆∞·ª£c t√≠nh b·∫±ng t·ªïng chi ti√™u chia cho s·ªë l∆∞·ª£ng giao d·ªãch.                                                                      |
| Unique_Products_Purchased  | S·ªë l∆∞·ª£ng s·∫£n ph·∫©m kh√°c nhau m√† kh√°ch h√†ng ƒë√£ mua.                                                                                                                    |
| Average_Days_Between_Purchases | S·ªë ng√†y trung b√¨nh gi·ªØa c√°c l·∫ßn mua h√†ng li√™n ti·∫øp c·ªßa kh√°ch h√†ng.                                                                                                  |
| Day_Of_Week                | Ng√†y trong tu·∫ßn m√† kh√°ch h√†ng th√≠ch mua h√†ng nh·∫•t (bi·ªÉu di·ªÖn b·∫±ng s·ªë, 0 l√† Th·ª© Hai, 6 l√† Ch·ªß Nh·∫≠t).                                                                  |
| Hour                       | Gi·ªù trong ng√†y m√† kh√°ch h√†ng th√≠ch mua h√†ng nh·∫•t (ƒë·ªãnh d·∫°ng 24 gi·ªù).                                                                                                  |
| Is_UK                      | Bi·∫øn nh·ªã ph√¢n x√°c ƒë·ªãnh kh√°ch h√†ng c√≥ ƒë·∫øn t·ª´ V∆∞∆°ng qu·ªëc Anh hay kh√¥ng (1 n·∫øu c√≥, 0 n·∫øu kh√¥ng).                                                                         |
| Cancellation_Frequency     | T·ªïng s·ªë giao d·ªãch m√† kh√°ch h√†ng ƒë√£ h·ªßy.                                                                                                                              |
| Cancellation_Rate          | T·ª∑ l·ªá h·ªßy ƒë∆°n, ƒë∆∞·ª£c t√≠nh b·∫±ng s·ªë giao d·ªãch h·ªßy chia cho t·ªïng s·ªë giao d·ªãch.                                                                                           |
| Monthly_Spending_Mean      | Chi ti√™u trung b√¨nh h√†ng th√°ng c·ªßa kh√°ch h√†ng.                                                                                                                       |
| Monthly_Spending_Std       | ƒê·ªô l·ªách chu·∫©n c·ªßa chi ti√™u h√†ng th√°ng c·ªßa kh√°ch h√†ng, ph·∫£n √°nh m·ª©c ƒë·ªô bi·∫øn ƒë·ªông trong h√†nh vi chi ti√™u.                                                               |
| Spending_Trend             | Gi√° tr·ªã s·ªë ph·∫£n √°nh xu h∆∞·ªõng chi ti√™u c·ªßa kh√°ch h√†ng theo th·ªùi gian: gi√° tr·ªã d∆∞∆°ng l√† tƒÉng, gi√° tr·ªã √¢m l√† gi·∫£m, g·∫ßn 0 l√† ·ªïn ƒë·ªãnh.                                     |

## Ch√∫ng ta ƒë√£ l√†m r·∫•t t·ªët cho ƒë·∫øn th·ªùi ƒëi·ªÉm n√†y!

Ch√∫ng ta ƒë√£ t·∫°o ra m·ªôt **t·∫≠p d·ªØ li·ªáu t·∫≠p trung v√†o kh√°ch h√†ng**, v·ªõi **nhi·ªÅu ƒë·∫∑c tr∆∞ng m·ªõi** gi√∫p ch√∫ng ta **hi·ªÉu s√¢u h∆°n v·ªÅ h√†nh vi mua s·∫Øm v√† s·ªü th√≠ch c·ªßa h·ªç**.

---

## B∆∞·ªõc ti·∫øp theo c·ªßa d·ª± √°n:

Gi·ªù ƒë√¢y khi t·∫≠p d·ªØ li·ªáu ƒë√£ **s·∫µn s√†ng**, ch√∫ng ta c√≥ th·ªÉ chuy·ªÉn sang c√°c b∆∞·ªõc ti·∫øp theo c·ªßa d·ª± √°n, bao g·ªìm:

- **Xem x√©t d·ªØ li·ªáu k·ªπ h∆°n** ƒë·ªÉ t√¨m ra c√°c **m·∫´u (patterns)** ho·∫∑c **xu h∆∞·ªõng (trends)**.
- **ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu** b·∫±ng c√°ch ki·ªÉm tra v√† x·ª≠ l√Ω c√°c **gi√° tr·ªã ngo·∫°i lai (outliers)**.
- **Chu·∫©n b·ªã d·ªØ li·ªáu** cho qu√° tr√¨nh **ph√¢n c·ª•m (clustering)**.

---

## √ù nghƒ©a:

T·∫•t c·∫£ c√°c b∆∞·ªõc n√†y s·∫Ω gi√∫p ch√∫ng ta:
‚úÖ X√¢y d·ª±ng m·ªôt **n·ªÅn t·∫£ng v·ªØng ch·∫Øc** ƒë·ªÉ t·∫°o ra c√°c **ph√¢n kh√∫c kh√°ch h√†ng c√≥ √Ω nghƒ©a**.  
‚úÖ V√† cu·ªëi c√πng l√† **ph√°t tri·ªÉn m·ªôt h·ªá th·ªëng g·ª£i √Ω c√° nh√¢n h√≥a hi·ªáu qu·∫£**, ph√π h·ª£p v·ªõi nhu c·∫ßu v√† h√†nh vi c·ªßa t·ª´ng nh√≥m kh√°ch h√†ng.

## Step 5 | Outlier Detection and Treatment
## Nh·∫≠n Di·ªán v√† X·ª≠ L√Ω Outliers

Trong ph·∫ßn n√†y, t√¥i s·∫Ω **nh·∫≠n di·ªán v√† x·ª≠ l√Ω c√°c gi√° tr·ªã ngo·∫°i lai (outliers)** trong t·∫≠p d·ªØ li·ªáu.

---

### Outliers l√† g√¨?
- Outlier l√† nh·ªØng **ƒëi·ªÉm d·ªØ li·ªáu kh√°c bi·ªát ƒë√°ng k·ªÉ** so v·ªõi ph·∫ßn l·ªõn c√°c ƒëi·ªÉm c√≤n l·∫°i trong t·∫≠p d·ªØ li·ªáu.
- Nh·ªØng ƒëi·ªÉm n√†y c√≥ th·ªÉ **l√†m sai l·ªách k·∫øt qu·∫£ ph√¢n t√≠ch**, ƒë·∫∑c bi·ªát l√† trong **ph√¢n c·ª•m K-means**, n∆°i m√† outliers c√≥ th·ªÉ **·∫£nh h∆∞·ªüng m·∫°nh ƒë·∫øn v·ªã tr√≠ c·ªßa c√°c t√¢m c·ª•m (centroids)**.

---

### T·∫°i sao c·∫ßn x·ª≠ l√Ω outliers?
‚úÖ Gi√∫p thu ƒë∆∞·ª£c k·∫øt qu·∫£ **ph√¢n c·ª•m ch√≠nh x√°c v√† c√≥ √Ω nghƒ©a h∆°n**.  
‚úÖ Tr√°nh **sai l·ªách** do c√°c gi√° tr·ªã b·∫•t th∆∞·ªùng g√¢y ra.

---

### Ph∆∞∆°ng ph√°p ph√°t hi·ªán outliers:
üîç V√¨ d·ªØ li·ªáu c·ªßa ch√∫ng ta c√≥ **nhi·ªÅu chi·ªÅu (multi-dimensional)**, n√™n s·∫Ω h·ª£p l√Ω h∆°n n·∫øu s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n c√≥ kh·∫£ nƒÉng ph√°t hi·ªán outliers trong kh√¥ng gian nhi·ªÅu chi·ªÅu.

---

### T√¥i s·∫Ω s·ª≠ d·ª•ng **Isolation Forest**:
- Thu·∫≠t to√°n n√†y ho·∫°t ƒë·ªông t·ªët v·ªõi **d·ªØ li·ªáu nhi·ªÅu chi·ªÅu**.
- C√≥ **hi·ªáu su·∫•t t√≠nh to√°n cao**.
- **C∆° ch·∫ø ho·∫°t ƒë·ªông**:
  - **C√¥ l·∫≠p ƒëi·ªÉm d·ªØ li·ªáu** b·∫±ng c√°ch ch·ªçn ng·∫´u nhi√™n m·ªôt ƒë·∫∑c tr∆∞ng.
  - Ch·ªçn **gi√° tr·ªã ph√¢n t√°ch ng·∫´u nhi√™n** gi·ªØa gi√° tr·ªã l·ªõn nh·∫•t v√† nh·ªè nh·∫•t c·ªßa ƒë·∫∑c tr∆∞ng ƒë√≥.

---

‚û° **H√£y c√πng tri·ªÉn khai ph∆∞∆°ng ph√°p n√†y nh√©!**

#ki·ªÉm tra c√≥ bao nhi√™u c·ªôt
df.shape[1]

#ki·ªÉm tra customer_data c√≥ bao nhi√™u c·ªôt c√≥ ki·ªÉu d·ªØ li·ªáu s·ªë
print("T·ªïng s·ªë chi·ªÅu s·ªë:", customer_data.select_dtypes(include='number').shape[1])
# Kh·ªüi t·∫°o m√¥ h√¨nh IsolationForest v·ªõi tham s·ªë contamination = 0.05 (t·ª©c l√† gi·∫£ ƒë·ªãnh 5% d·ªØ li·ªáu l√† outlier)
model = IsolationForest(contamination=0.05, random_state=0)

# Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n d·ªØ li·ªáu (chuy·ªÉn DataFrame th√†nh NumPy ƒë·ªÉ tr√°nh c·∫£nh b√°o)
customer_data['Outlier_Scores'] = model.fit_predict(customer_data.iloc[:, 1:].to_numpy())

# T·∫°o c·ªôt m·ªõi 'Is_Outlier' ƒë·ªÉ d·ªÖ hi·ªÉu h∆°n: 1 n·∫øu l√† outlier, 0 n·∫øu kh√¥ng ph·∫£i
customer_data['Is_Outlier'] = [1 if x == -1 else 0 for x in customer_data['Outlier_Scores']]
# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n sau khi th√™m th√¥ng tin outlier
customer_data.head()

## Tr·ª±c Quan H√≥a K·∫øt Qu·∫£ Ph√°t Hi·ªán Outliers

Sau khi √°p d·ª•ng thu·∫≠t to√°n **Isolation Forest**, ch√∫ng ta ƒë√£:

‚úÖ **X√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c gi√° tr·ªã ngo·∫°i lai (outliers)** v√† ƒë√°nh d·∫•u ch√∫ng trong m·ªôt c·ªôt m·ªõi c√≥ t√™n l√† **Is_Outlier**.

‚úÖ T√≠nh to√°n ƒë∆∞·ª£c **ƒëi·ªÉm b·∫•t th∆∞·ªùng (outlier scores)**, ph·∫£n √°nh **m·ª©c ƒë·ªô b·∫•t th∆∞·ªùng** c·ªßa t·ª´ng b·∫£n ghi trong t·∫≠p d·ªØ li·ªáu.

---

### üìä Ti·∫øp theo:
- Tr·ª±c quan h√≥a **ph√¢n b·ªë c·ªßa c√°c ƒëi·ªÉm s·ªë b·∫•t th∆∞·ªùng** (outlier scores).
- Tr·ª±c quan h√≥a **s·ªë l∆∞·ª£ng ƒëi·ªÉm b√¨nh th∆∞·ªùng (inliers)** v√† **ƒëi·ªÉm ngo·∫°i lai (outliers)** m√† m√¥ h√¨nh ƒë√£ ph√°t hi·ªán.

---

üéØ Vi·ªác tr·ª±c quan h√≥a n√†y gi√∫p:
- Hi·ªÉu r√µ **ƒë·∫∑c ƒëi·ªÉm ph√¢n b·ªë outliers**.
- X√°c ƒë·ªãnh xem outliers xu·∫•t hi·ªán ·ªü m·ª©c ƒë·ªô n√†o so v·ªõi ph·∫ßn c√≤n l·∫°i c·ªßa d·ªØ li·ªáu.

# T√≠nh % c·ªßa inliers v√† outliers
outlier_percentage = customer_data['Is_Outlier'].value_counts(normalize=True) * 100

plt.figure(figsize=(12, 4))
outlier_percentage.plot(kind='barh', color='#ff6200')


for index, value in enumerate(outlier_percentage):
    plt.text(value, index, f'{value:.2f}%', fontsize=15)

plt.title('Percentage of Inliers and Outliers')
plt.xticks(ticks=np.arange(0, 115, 5))
plt.xlabel('Percentage (%)')
plt.ylabel('Is Outlier')
plt.gca().invert_yaxis()
plt.show()
## Nh·∫≠n ƒë·ªãnh:

T·ª´ bi·ªÉu ƒë·ªì ·ªü tr√™n, ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng **kho·∫£ng 5% kh√°ch h√†ng** ƒë√£ ƒë∆∞·ª£c x√°c ƒë·ªãnh l√† **outliers** trong t·∫≠p d·ªØ li·ªáu.

---

### T·ª∑ l·ªá n√†y c√≥ v·∫ª h·ª£p l√Ω:
- **Kh√¥ng qu√° cao** ƒë·∫øn m·ª©c lo·∫°i b·ªè nhi·ªÅu d·ªØ li·ªáu quan tr·ªçng.
- **C≈©ng kh√¥ng qu√° th·∫•p** ƒë·∫øn m·ª©c gi·ªØ l·∫°i nhi·ªÅu ƒëi·ªÉm nhi·ªÖu.

---

### K·∫øt lu·∫≠n:
‚úÖ ƒêi·ªÅu n√†y cho th·∫•y **thu·∫≠t to√°n Isolation Forest** ƒë√£ ho·∫°t ƒë·ªông hi·ªáu qu·∫£, x√°c ƒë·ªãnh ƒë∆∞·ª£c **m·ªôt t·ª∑ l·ªá v·ª´a ph·∫£i** c√°c ngo·∫°i l·ªá.  
‚úÖ Vi·ªác n√†y **r·∫•t quan tr·ªçng** ƒë·ªÉ n√¢ng cao **ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh ph√¢n c·ª•m kh√°ch h√†ng** sau n√†y.
## Chi·∫øn l∆∞·ª£c:

X√©t trong b·ªëi c·∫£nh c·ªßa d·ª± √°n (**ph√¢n c·ª•m kh√°ch h√†ng b·∫±ng clustering**), vi·ªác **x·ª≠ l√Ω outliers** l√† **r·∫•t quan tr·ªçng** ƒë·ªÉ:

- NgƒÉn **outliers l√†m sai l·ªách c√°c c·ª•m ph√¢n nh√≥m**.
- TƒÉng **ch·∫•t l∆∞·ª£ng** v√† **ƒë·ªô ch√≠nh x√°c** c·ªßa k·∫øt qu·∫£ ph√¢n c·ª•m.

---

### C√°c b∆∞·ªõc th·ª±c hi·ªán:

1Ô∏è‚É£ **T√°ch ri√™ng c√°c outliers** ƒë·ªÉ ph·ª•c v·ª• cho **ph√¢n t√≠ch chuy√™n s√¢u sau n√†y** (c√≥ th·ªÉ **l∆∞u ra file ri√™ng** ‚Äì t√πy ch·ªçn).

2Ô∏è‚É£ **Lo·∫°i b·ªè c√°c outliers** kh·ªèi t·∫≠p d·ªØ li·ªáu ch√≠nh ƒë·ªÉ chu·∫©n b·ªã cho qu√° tr√¨nh ph√¢n c·ª•m.

3Ô∏è‚É£ **X√≥a c√°c c·ªôt ph·ª• tr·ª£** (`Outlier_Scores` v√† `Is_Outlier`) v√¨ ch√∫ng ch·ªâ ƒë∆∞·ª£c d√πng cho b∆∞·ªõc ph√°t hi·ªán outlier.

---

üéØ **H√£y c√πng th·ª±c hi·ªán c√°c b∆∞·ªõc n√†y nh√©!**

# T√°ch c√°c d√≤ng ƒë∆∞·ª£c ƒë√°nh d·∫•u l√† outlier ra ƒë·ªÉ ph√¢n t√≠ch ri√™ng
outliers_data = customer_data[customer_data['Is_Outlier'] == 1]
# Lo·∫°i b·ªè c√°c outliers kh·ªèi t·∫≠p d·ªØ li·ªáu ch√≠nh ƒë·ªÉ chu·∫©n b·ªã cho b∆∞·ªõc ph√¢n c·ª•m
customer_data_cleaned = customer_data[customer_data['Is_Outlier'] == 0]
# X√≥a c√°c c·ªôt ph·ª• tr·ª£ d√πng ƒë·ªÉ x√°c ƒë·ªãnh outliers v√¨ kh√¥ng c√≤n c·∫ßn thi·∫øt n·ªØa
customer_data_cleaned = customer_data_cleaned.drop(columns=['Outlier_Scores', 'Is_Outlier'])
# ƒê·∫∑t l·∫°i ch·ªâ s·ªë d√≤ng cho b·∫£ng d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
customer_data_cleaned.reset_index(drop=True, inplace=True)
## K·∫øt qu·∫£ x·ª≠ l√Ω Outliers

Ch√∫ng ta ƒë√£ **t√°ch th√†nh c√¥ng c√°c gi√° tr·ªã ngo·∫°i lai (outliers)** ƒë·ªÉ **ph√¢n t√≠ch ri√™ng**, ƒë·ªìng th·ªùi **l√†m s·∫°ch t·∫≠p d·ªØ li·ªáu ch√≠nh** b·∫±ng c√°ch lo·∫°i b·ªè nh·ªØng outliers n√†y.

---

### T·∫≠p d·ªØ li·ªáu sau khi l√†m s·∫°ch hi·ªán ƒë√£ s·∫µn s√†ng cho c√°c b∆∞·ªõc ti·∫øp theo trong d·ª± √°n ph√¢n kh√∫c kh√°ch h√†ng, bao g·ªìm:

‚úÖ **Chu·∫©n h√≥a c√°c ƒë·∫∑c tr∆∞ng (scaling features)**.

‚úÖ **√Åp d·ª•ng thu·∫≠t to√°n ph√¢n c·ª•m (clustering)** ƒë·ªÉ x√°c ƒë·ªãnh c√°c **nh√≥m kh√°ch h√†ng kh√°c bi·ªát**.

# Getting the number of rows in the cleaned customer dataset
customer_data_cleaned.shape[0]
## Step 6 | Feature Scaling
## T·∫°i sao c·∫ßn chu·∫©n h√≥a (Scaling) d·ªØ li·ªáu tr∆∞·ªõc khi ph√¢n c·ª•m v√† gi·∫£m chi·ªÅu?

Tr∆∞·ªõc khi ti·∫øn h√†nh c√°c b∆∞·ªõc **ph√¢n c·ª•m (clustering)** v√† **gi·∫£m chi·ªÅu (dimensionality reduction)**, vi·ªác **chu·∫©n h√≥a (scale)** c√°c ƒë·∫∑c tr∆∞ng l√† ƒëi·ªÅu **b·∫Øt bu·ªôc**.

B∆∞·ªõc n√†y **ƒë·∫∑c bi·ªát quan tr·ªçng** khi l√†m vi·ªác v·ªõi c√°c thu·∫≠t to√°n **d·ª±a tr√™n kho·∫£ng c√°ch** nh∆∞ **K-means** v√† c√°c ph∆∞∆°ng ph√°p gi·∫£m chi·ªÅu nh∆∞ **PCA**.

---

### L√Ω do:

---

### üîπ ƒê·ªëi v·ªõi ph√¢n c·ª•m K-means:
- **K-means d·ª±a r·∫•t nhi·ªÅu v√†o kh√°i ni·ªám "kho·∫£ng c√°ch"** gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu ƒë·ªÉ h√¨nh th√†nh c√°c c·ª•m.
- N·∫øu c√°c ƒë·∫∑c tr∆∞ng kh√¥ng ƒë∆∞·ª£c ƒë∆∞a v·ªÅ c√πng **thang ƒëo**, nh·ªØng ƒë·∫∑c tr∆∞ng c√≥ **gi√° tr·ªã l·ªõn h∆°n** s·∫Ω chi ph·ªëi k·∫øt qu·∫£ ph√¢n c·ª•m.
- ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn **k·∫øt qu·∫£ ph√¢n nh√≥m kh√¥ng ch√≠nh x√°c**.

---

### üîπ ƒê·ªëi v·ªõi PCA:
- **PCA t√¨m c√°c h∆∞·ªõng (th√†nh ph·∫ßn ch√≠nh)** m√† trong ƒë√≥ d·ªØ li·ªáu c√≥ ƒë·ªô bi·∫øn thi√™n l·ªõn nh·∫•t.
- N·∫øu kh√¥ng chu·∫©n h√≥a, c√°c ƒë·∫∑c tr∆∞ng c√≥ **gi√° tr·ªã l·ªõn h∆°n** s·∫Ω chi ph·ªëi c√°c th√†nh ph·∫ßn ch√≠nh n√†y.
- K·∫øt qu·∫£ s·∫Ω kh√¥ng ph·∫£n √°nh ƒë√∫ng **c·∫•u tr√∫c ti·ªÅm ·∫©n c·ªßa d·ªØ li·ªáu**.

---

‚úÖ V√¨ v·∫≠y, vi·ªác **chu·∫©n h√≥a d·ªØ li·ªáu** tr∆∞·ªõc khi th·ª±c hi·ªán K-means v√† PCA l√† **r·∫•t quan tr·ªçng** ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh **c√¥ng b·∫±ng** gi·ªØa c√°c ƒë·∫∑c tr∆∞ng v√† **ƒë·ªô ch√≠nh x√°c** c·ªßa k·∫øt qu·∫£ ph√¢n t√≠ch.

## Ph∆∞∆°ng ph√°p chu·∫©n h√≥a d·ªØ li·ªáu (Methodology)

ƒê·ªÉ ƒë·∫£m b·∫£o c√°c ƒë·∫∑c tr∆∞ng c√≥ **·∫£nh h∆∞·ªüng c√¢n b·∫±ng** l√™n m√¥ h√¨nh v√† kh√°m ph√° ƒë∆∞·ª£c **c√°c m·∫´u ti·ªÅm ·∫©n th·ª±c s·ª±** trong d·ªØ li·ªáu, t√¥i s·∫Ω ti·∫øn h√†nh **chu·∫©n h√≥a (standardize)** d·ªØ li·ªáu, nghƒ©a l√†:

- Chuy·ªÉn ƒë·ªïi c√°c ƒë·∫∑c tr∆∞ng sao cho c√≥:
  - Trung b√¨nh = 0
  - ƒê·ªô l·ªách chu·∫©n = 1

---

### Tuy nhi√™n, kh√¥ng ph·∫£i t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng ƒë·ªÅu c·∫ßn ƒë∆∞·ª£c chu·∫©n h√≥a. D∆∞·ªõi ƒë√¢y l√† c√°c ngo·∫°i l·ªá v√† l√Ω do lo·∫°i tr·ª´:

---

### ‚ùå Kh√¥ng chu·∫©n h√≥a:

- **CustomerID**:  
  - ƒê√¢y ch·ªâ l√† m√£ ƒë·ªãnh danh kh√°ch h√†ng, kh√¥ng mang √Ω nghƒ©a ph√¢n t√≠ch cho b√†i to√°n clustering.

- **Is_UK**:  
  - L√† m·ªôt bi·∫øn nh·ªã ph√¢n (0 ho·∫∑c 1) bi·ªÉu th·ªã kh√°ch h√†ng c√≥ ƒë·∫øn t·ª´ UK hay kh√¥ng.
  - Do ƒë√£ ·ªü d·∫°ng nh·ªã ph√¢n, vi·ªác chu·∫©n h√≥a s·∫Ω **kh√¥ng mang l·∫°i √Ω nghƒ©a** ƒë√°ng k·ªÉ.

- **Day_Of_Week**:  
  - ƒê·∫°i di·ªán cho **ng√†y trong tu·∫ßn** m√† kh√°ch h√†ng th∆∞·ªùng mua h√†ng nh·∫•t (1‚Äì7).
  - ƒê√¢y l√† m·ªôt ƒë·∫∑c tr∆∞ng d·∫°ng **categorical d·∫°ng s·ªë**, kh√¥ng c·∫ßn chu·∫©n h√≥a.

---

‚úÖ T√¥i s·∫Ω chu·∫©n h√≥a **c√°c ƒë·∫∑c tr∆∞ng c√≤n l·∫°i** trong dataset ƒë·ªÉ chu·∫©n b·ªã cho c√°c b∆∞·ªõc ti·∫øp theo: **PCA** v√† **K-means clustering**.

# Kh·ªüi t·∫°o b·ªô chu·∫©n h√≥a StandardScaler
scaler = StandardScaler()

# Danh s√°ch c√°c c·ªôt kh√¥ng c·∫ßn chu·∫©n h√≥a
columns_to_exclude = ['CustomerID', 'Is_UK', 'Day_Of_Week']

# Danh s√°ch c√°c c·ªôt c·∫ßn ƒë∆∞·ª£c chu·∫©n h√≥a
columns_to_scale = customer_data_cleaned.columns.difference(columns_to_exclude)

# Sao ch√©p t·∫≠p d·ªØ li·ªáu ƒë√£ l√†m s·∫°ch
customer_data_scaled = customer_data_cleaned.copy()

# √Åp d·ª•ng b·ªô chu·∫©n h√≥a cho c√°c c·ªôt c·∫ßn thi·∫øt trong t·∫≠p d·ªØ li·ªáu
customer_data_scaled[columns_to_scale] = scaler.fit_transform(customer_data_scaled[columns_to_scale])

# Hi·ªÉn th·ªã v√†i d√≤ng ƒë·∫ßu ti√™n c·ªßa d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a
customer_data_scaled.head()

L√∫c n√†y customer_data_scaled c√≥ c·ªôt d·∫°ng Object ho·∫∑c String => ch·ªâ ch·ªçn c√°c c·ªôt numeric c·∫ßn thi·∫øt
## Step 7 | K-Means Clustering
## Gi·ªõi thi·ªáu v·ªÅ K-Means

**K-Means** l√† m·ªôt thu·∫≠t to√°n **h·ªçc m√°y kh√¥ng gi√°m s√°t (unsupervised learning)** d√πng ƒë·ªÉ **ph√¢n c·ª•m d·ªØ li·ªáu** th√†nh m·ªôt s·ªë l∆∞·ª£ng nh√≥m **x√°c ƒë·ªãnh tr∆∞·ªõc** (K c·ª•m), b·∫±ng c√°ch **t·ªëi thi·ªÉu h√≥a t·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch trong c·ª•m** (**WCSS ‚Äì Within-Cluster Sum of Squares**, hay c√≤n g·ªçi l√† **inertia**).

---

### Nguy√™n l√Ω ho·∫°t ƒë·ªông:
1Ô∏è‚É£ **G√°n m·ªói ƒëi·ªÉm d·ªØ li·ªáu** v√†o **t√¢m c·ª•m g·∫ßn nh·∫•t** (centroid).  
2Ô∏è‚É£ **C·∫≠p nh·∫≠t l·∫°i v·ªã tr√≠ c·ªßa c√°c t√¢m c·ª•m** b·∫±ng c√°ch t√≠nh **trung b√¨nh** c√°c ƒëi·ªÉm thu·ªôc c·ª•m ƒë√≥.  
3Ô∏è‚É£ L·∫∑p l·∫°i qu√° tr√¨nh tr√™n cho ƒë·∫øn khi thu·∫≠t to√°n **h·ªôi t·ª•** (centroids kh√¥ng thay ƒë·ªïi n·ªØa) ho·∫∑c ƒë·∫°t **ƒëi·ªÅu ki·ªán d·ª´ng**.

---

‚úÖ **M·ª•c ti√™u cu·ªëi c√πng**:  
T√¨m ra c√°c **c·ª•m d·ªØ li·ªáu** sao cho **kho·∫£ng c√°ch gi·ªØa c√°c ƒëi·ªÉm trong c√πng c·ª•m l√† nh·ªè nh·∫•t**, c√≤n **kho·∫£ng c√°ch gi·ªØa c√°c c·ª•m l√† l·ªõn nh·∫•t**.

## Step 8.1 | Determining the Optimal Number of Clusters
## X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng c·ª•m t·ªëi ∆∞u (k) trong ph√¢n kh√∫c kh√°ch h√†ng

ƒê·ªÉ x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng c·ª•m **t·ªëi ∆∞u (k)** trong vi·ªác ph√¢n kh√∫c kh√°ch h√†ng, t√¥i s·∫Ω s·ª≠ d·ª•ng **hai ph∆∞∆°ng ph√°p n·ªïi ti·∫øng** sau:

---

### üîπ Ph∆∞∆°ng ph√°p Elbow (Elbow Method):
- Xem x√©t m·ªëi quan h·ªá gi·ªØa s·ªë l∆∞·ª£ng c·ª•m **k** v√† **WCSS (Within-Cluster Sum of Squares)**.
- ƒêi·ªÉm **"elbow"** (khi ƒë·ªô gi·∫£m WCSS b·∫Øt ƒë·∫ßu ch·∫≠m l·∫°i) ƒë∆∞·ª£c xem l√† **k t·ªëi ∆∞u**.

# Step 8.1.1 | Elbow Method
## Elbow Method l√† g√¨?

**Elbow Method** (ph∆∞∆°ng ph√°p khu·ª∑u tay) l√† m·ªôt k·ªπ thu·∫≠t d√πng ƒë·ªÉ **x√°c ƒë·ªãnh s·ªë l∆∞·ª£ng c·ª•m l√Ω t∆∞·ªüng** trong m·ªôt t·∫≠p d·ªØ li·ªáu.  
Ph∆∞∆°ng ph√°p n√†y th·ª±c hi·ªán nh∆∞ sau:

---

### C√°ch th·ª±c hi·ªán:
1Ô∏è‚É£ **L·∫∑p qua nhi·ªÅu gi√° tr·ªã kh√°c nhau c·ªßa k** (s·ªë c·ª•m c·∫ßn ph√¢n lo·∫°i).  
2Ô∏è‚É£ V·ªõi m·ªói gi√° tr·ªã k, thu·∫≠t to√°n **K-means** s·∫Ω t√≠nh:
- **T·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch** gi·ªØa t·ª´ng ƒëi·ªÉm d·ªØ li·ªáu v√† t√¢m c·ª•m ƒë∆∞·ª£c g√°n (g·ªçi l√† **inertia** ho·∫∑c **WCSS ‚Äì Within-Cluster Sum of Squares**).
3Ô∏è‚É£ Sau ƒë√≥, v·∫Ω **bi·ªÉu ƒë·ªì gi·ªØa inertia v√† s·ªë c·ª•m k**.
- ƒê∆∞·ªùng cong thu ƒë∆∞·ª£c th∆∞·ªùng c√≥ h√¨nh d·∫°ng gi·ªëng nh∆∞ **khu·ª∑u tay** (elbow), v√¨ th·∫ø ƒë∆∞·ª£c g·ªçi l√† **Elbow Method**.

---

### üëâ ƒêi·ªÉm khu·ª∑u tay (elbow point):
- Ch√≠nh l√† gi√° tr·ªã **k** m√† t·ª´ ƒë√≥, vi·ªác tƒÉng th√™m c·ª•m **kh√¥ng gi√∫p gi·∫£m ƒë√°ng k·ªÉ inertia n·ªØa**.
- ƒê√¢y ƒë∆∞·ª£c xem l√† **s·ªë c·ª•m t·ªëi ∆∞u c·∫ßn ch·ªçn**.

---

‚úÖ Ph∆∞∆°ng ph√°p n√†y gi√∫p ch·ªçn s·ªë c·ª•m k **h·ª£p l√Ω**, tr√°nh vi·ªác **ph√¢n c·ª•m qu√° √≠t** ho·∫∑c **qu√° nhi·ªÅu**, ƒë·∫£m b·∫£o hi·ªáu qu·∫£ ph√¢n t√≠ch v√† d·ªÖ di·ªÖn gi·∫£i.
## S·ª≠ d·ª•ng Th∆∞ vi·ªán YellowBrick

Trong ph·∫ßn n√†y, t√¥i s·∫Ω s·ª≠ d·ª•ng th∆∞ vi·ªán **YellowBrick** ƒë·ªÉ h·ªó tr·ª£ tri·ªÉn khai **ph∆∞∆°ng ph√°p Elbow**.

---

### YellowBrick l√† g√¨?
- **YellowBrick** l√† m·ªôt **th∆∞ vi·ªán m·ªü r·ªông** c·ªßa **Scikit-Learn API**.
- Th∆∞ vi·ªán n√†y n·ªïi ti·∫øng nh·ªù kh·∫£ nƒÉng **t·∫°o ra c√°c tr·ª±c quan h√≥a nhanh ch√≥ng v√† tr·ª±c quan** trong lƒ©nh v·ª±c **h·ªçc m√°y (machine learning)**.
- ƒê·∫∑c bi·ªát h·ªØu √≠ch trong vi·ªác:
  - Kh√°m ph√° d·ªØ li·ªáu.
  - Hi·ªÉu r√µ h∆°n v·ªÅ k·∫øt qu·∫£ c·ªßa c√°c m√¥ h√¨nh h·ªçc m√°y.
  - Tr·ª±c quan h√≥a c√°c ch·ªâ s·ªë, ph√¢n c·ª•m v√† bi·ªÉu ƒë·ªì.

---

‚úÖ Vi·ªác s·ª≠ d·ª•ng **YellowBrick** gi√∫p:
- Ti·∫øt ki·ªám th·ªùi gian.
- TƒÉng t√≠nh tr·ª±c quan khi th·ª±c hi·ªán **ph√¢n c·ª•m K-means** v√† t√¨m s·ªë c·ª•m t·ªëi ∆∞u b·∫±ng **Elbow Method**.

# Thi·∫øt l·∫≠p ki·ªÉu bi·ªÉu ƒë·ªì v√† m√†u n·ªÅn tr·ª•c
sns.set(style='darkgrid', rc={'axes.facecolor': '#fcf0dc'})
sns.set_palette(['#ff6200'])

# Ch·ªçn c·ªôt numeric c·∫ßn thi·∫øt ƒë·ªÉ ƒë∆∞a v√†o KMeans
columns_to_use = customer_data_scaled.select_dtypes(include=[np.number]).columns.tolist()

# Lo·∫°i b·ªè c·ªôt CustomerID n·∫øu c√≥
if 'CustomerID' in columns_to_use:
    columns_to_use.remove('CustomerID')

# Kh·ªüi t·∫°o m√¥ h√¨nh ph√¢n c·ª•m KMeans v·ªõi c√°c tham s·ªë ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
km = KMeans(init='k-means++', n_init=10, max_iter=100, random_state=0)

# T·∫°o ƒë·ªëi t∆∞·ª£ng bi·ªÉu ƒë·ªì v√† tr·ª•c v·ªõi k√≠ch th∆∞·ªõc mong mu·ªën
fig, ax = plt.subplots(figsize=(12, 5))

# Kh·ªüi t·∫°o KElbowVisualizer v·ªõi m√¥ h√¨nh v√† ph·∫°m vi gi√° tr·ªã k, t·∫Øt hi·ªÉn th·ªã th·ªùi gian hu·∫•n luy·ªán
visualizer = KElbowVisualizer(km, k=(2, 15), timings=False, ax=ax)

# Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi d·ªØ li·ªáu ƒë√£ ch·ªçn ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì Elbow
visualizer.fit(customer_data_scaled[columns_to_use])

# Ho√†n t·∫•t v√† hi·ªÉn th·ªã bi·ªÉu ƒë·ªì
visualizer.show()
## K·∫øt Qu·∫£ Ph√¢n T√≠ch Elbow ‚Äì X√°c ƒê·ªãnh Gi√° Tr·ªã k T·ªëi ∆Øu

Bi·ªÉu ƒë·ªì Elbow cho th·∫•y **gi√° tr·ªã k ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t l√† 5** (Elbow Point t·∫°i k=5, Distortion Score ~40,550).

---

### Nh·∫≠n x√©t:
- **ƒêi·ªÉm khu·ª∑u tay kh√¥ng qu√° r√µ r√†ng** (elbow kh√¥ng s·∫Øc n√©t) ‚Äî ƒëi·ªÅu n√†y **th∆∞·ªùng g·∫∑p khi x·ª≠ l√Ω d·ªØ li·ªáu th·ª±c t·∫ø**.
- Quan s√°t ƒë∆∞·ªùng gi·∫£m distortion, ta th·∫•y:
  - ƒê·ªô d·ªëc b·∫Øt ƒë·∫ßu gi·∫£m ch·∫≠m l·∫°i sau **k=5**.
  - Tuy nhi√™n, distortion v·∫´n ti·∫øp t·ª•c gi·∫£m ƒë·ªÅu.
- Do ƒë√≥, **k=5 l√† l·ª±a ch·ªçn h·ª£p l√Ω**, nh∆∞ng c≈©ng c√≥ th·ªÉ c√¢n nh·∫Øc c√°c gi√° tr·ªã **k kh√°c trong kho·∫£ng 3 ‚â§ k ‚â§ 7** ƒë·ªÉ ki·ªÉm tra th√™m.

---

### Khuy·∫øn ngh·ªã:
‚úÖ **D√πng ph√¢n t√≠ch silhouette** ƒë·ªÉ ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c·ª•m trong kho·∫£ng **k** n√†y.  
‚úÖ **K·∫øt h·ª£p th√™m business insights (hi·ªÉu bi·∫øt nghi·ªáp v·ª•)** ƒë·ªÉ x√°c ƒë·ªãnh **s·ªë c·ª•m th·ª±c t·∫ø ph√π h·ª£p nh·∫•t** v·ªõi b√†i to√°n.

---

üîç B∆∞·ªõc ti·∫øp theo: Ti·∫øn h√†nh ph√¢n t√≠ch **Silhouette** ƒë·ªÉ ch·ªçn **k t·ªët nh·∫•t**.

# Step 8.2 | Clustering Model - K-means
## √Åp D·ª•ng Thu·∫≠t To√°n Ph√¢n C·ª•m K-Means

Trong b∆∞·ªõc n√†y, t√¥i s·∫Ω **√°p d·ª•ng thu·∫≠t to√°n ph√¢n c·ª•m K-means** ƒë·ªÉ **ph√¢n kh√∫c kh√°ch h√†ng** th√†nh c√°c nh√≥m kh√°c nhau, d·ª±a tr√™n:
- **H√†nh vi mua h√†ng**.
- C√°c **ƒë·∫∑c ƒëi·ªÉm li√™n quan kh√°c**.

‚úÖ S·ªë l∆∞·ª£ng c·ª•m t·ªëi ∆∞u (**k**) ƒë√£ ƒë∆∞·ª£c x√°c ƒë·ªãnh ·ªü b∆∞·ªõc tr∆∞·ªõc.

---

### L∆∞u √Ω quan tr·ªçng:
- **Thu·∫≠t to√°n K-means** c√≥ th·ªÉ **g√°n nh√£n c·ª•m kh√°c nhau** trong m·ªói l·∫ßn ch·∫°y, do t√≠nh ch·∫•t **kh·ªüi t·∫°o ng·∫´u nhi√™n ban ƒë·∫ßu**.
- ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y, t√¥i ƒë√£ th·ª±c hi·ªán m·ªôt b∆∞·ªõc b·ªï sung:
  - **S·∫Øp x·∫øp l·∫°i nh√£n c·ª•m theo t·∫ßn su·∫•t xu·∫•t hi·ªán** (s·ªë l∆∞·ª£ng ƒëi·ªÉm trong m·ªói c·ª•m).
  - M·ª•c ƒë√≠ch: ƒê·∫£m b·∫£o **g√°n nh√£n c·ª•m nh·∫•t qu√°n** gi·ªØa c√°c l·∫ßn ch·∫°y kh√°c nhau.

---

‚úÖ Sau khi ph√¢n c·ª•m, ch√∫ng ta s·∫Ω c√≥ c√°c **nh√≥m kh√°ch h√†ng r√µ r√†ng** d·ª±a tr√™n **h√†nh vi v√† ƒë·∫∑c ƒëi·ªÉm** c·ªßa h·ªç, t·ª´ ƒë√≥ h·ªó tr·ª£ cho vi·ªác **x√¢y d·ª±ng chi·∫øn l∆∞·ª£c marketing** v√† **h·ªá th·ªëng g·ª£i √Ω c√° nh√¢n h√≥a**.

from sklearn.cluster import KMeans
from collections import Counter
import numpy as np

# 1Ô∏è‚É£ Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh KMeans v·ªõi k=3
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=100, random_state=0)
kmeans.fit(customer_data_scaled[columns_to_use])

# 2Ô∏è‚É£ L·∫•y t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa t·ª´ng c·ª•m
cluster_frequencies = Counter(kmeans.labels_)

# 3Ô∏è‚É£ T·∫°o √°nh x·∫° nh√£n c≈© ‚Üí nh√£n m·ªõi theo t·∫ßn su·∫•t (c·ª•m ƒë√¥ng nh·∫•t ‚Üí 0, ti·∫øp theo ‚Üí 1, ...)
label_mapping = {label: new_label for new_label, (label, _) in 
                 enumerate(cluster_frequencies.most_common())}

# 4Ô∏è‚É£ (Tu·ª≥ ch·ªçn) √Ånh x·∫° ng∆∞·ª£c l·∫°i theo √Ω ri√™ng c·ªßa b·∫°n (vd: ƒë·ªïi 0 -> 2, 1 -> 0, 2 -> 1)
custom_mapping = {2: 1, 1: 0, 0: 2}  # V√≠ d·ª•: tu·ª≥ ch·ªânh theo nhu c·∫ßu ph√¢n t√≠ch
final_mapping = {old_label: custom_mapping[new_label] for old_label, new_label in label_mapping.items()}

# 5Ô∏è‚É£ √Åp d·ª•ng √°nh x·∫°
new_labels = np.array([final_mapping[label] for label in kmeans.labels_])

# 6Ô∏è‚É£ Th√™m c·ªôt cluster v√†o dataset g·ªëc ƒë√£ l√†m s·∫°ch
customer_data_cleaned = customer_data_cleaned.copy()
customer_data_cleaned['cluster'] = new_labels

# 7Ô∏è‚É£ T·∫°o b·∫£ng k·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß (CustomerID + t·∫•t c·∫£ feature + cluster)
final_result = customer_data_cleaned.copy()

# 8Ô∏è‚É£ Xem k·∫øt qu·∫£
import pandas as pd
pd.set_option('display.max_columns', None)  # Hi·ªán ƒë·∫ßy ƒë·ªß c·ªôt
final_result.head()

## Step 9 | Clustering Evaluation
## ƒê√°nh Gi√° Ph√¢n C·ª•m ‚Äì Ch·ªâ S·ª≠ D·ª•ng Bi·ªÉu ƒê·ªì Ph√¢n B·ªë C·ª•m

Sau khi x√°c ƒë·ªãnh s·ªë c·ª•m t·ªëi ∆∞u (trong tr∆∞·ªùng h·ª£p n√†y l√† **3**) b·∫±ng **ph∆∞∆°ng ph√°p Elbow**, t√¥i ti·∫øn h√†nh b∆∞·ªõc **ƒë√°nh gi√° ph√¢n c·ª•m** ƒë·ªÉ ki·ªÉm tra **ch·∫•t l∆∞·ª£ng c·ªßa c√°c c·ª•m ƒë√£ h√¨nh th√†nh**.

---

### üéØ M·ª•c ti√™u c·ªßa b∆∞·ªõc n√†y:
‚úÖ **X√°c nh·∫≠n t√≠nh hi·ªáu qu·∫£** c·ªßa qu√° tr√¨nh ph√¢n c·ª•m.  
‚úÖ ƒê·∫£m b·∫£o c√°c c·ª•m ƒë∆∞·ª£c t·∫°o ra l√† **r√µ r√†ng**, **t√°ch bi·ªát** v√† **c√¢n b·∫±ng h·ª£p l√Ω** ƒë·ªÉ ph·ª•c v·ª• c√°c ph√¢n t√≠ch ti·∫øp theo.

---

### üîç Ph∆∞∆°ng ph√°p ƒê√°nh Gi√°:

- T√¥i t·∫≠p trung v√†o vi·ªác s·ª≠ d·ª•ng **Bi·ªÉu ƒë·ªì Ph√¢n B·ªë C·ª•m (Cluster Distribution)** ƒë·ªÉ xem x√©t **s·ªë l∆∞·ª£ng kh√°ch h√†ng trong t·ª´ng c·ª•m**.
- **Bi·ªÉu ƒë·ªì n√†y gi√∫p**:
  - X√°c ƒë·ªãnh xem c√°c c·ª•m c√≥ **ph√¢n b·ªï h·ª£p l√Ω** kh√¥ng.
  - Ph√°t hi·ªán c·ª•m n√†o **qu√° l·ªõn** ho·∫∑c **qu√° nh·ªè**, g√¢y **m·∫•t c√¢n ƒë·ªëi**.

---

### ‚ö†Ô∏è L∆∞u √Ω:
- T√¥i **kh√¥ng s·ª≠ d·ª•ng c√°c ch·ªâ s·ªë ƒë√°nh gi√°** nh∆∞ **Silhouette Score**, **Calinski-Harabasz** hay **Davies-Bouldin** trong b∆∞·ªõc n√†y.
- **Bi·ªÉu ƒë·ªì ph√¢n b·ªë c·ª•m** mang l·∫°i c√°i nh√¨n **nhanh v√† tr·ª±c quan** v·ªÅ c√°ch d·ªØ li·ªáu ƒë∆∞·ª£c ph√¢n chia gi·ªØa c√°c c·ª•m.
- Vi·ªác x√°c ƒë·ªãnh **s·ªë c·ª•m cu·ªëi c√πng** c≈©ng d·ª±a tr√™n **hi·ªÉu bi·∫øt nghi·ªáp v·ª•**, ƒë·∫£m b·∫£o ph√π h·ª£p v·ªõi **th·ª±c t·∫ø** v√† h·ªó tr·ª£ **ra quy·∫øt ƒë·ªãnh kinh doanh ch√≠nh x√°c**.

---

‚úÖ ƒê√¢y l√† b∆∞·ªõc **ƒë√°nh gi√° ban ƒë·∫ßu quan tr·ªçng** tr∆∞·ªõc khi ƒëi s√¢u v√†o ph√¢n t√≠ch chi ti·∫øt t·ª´ng c·ª•m.

## Step 10 | Cluster Distribution Visualization
T√¥i s·∫Ω s·ª≠ d·ª•ng **bi·ªÉu ƒë·ªì c·ªôt (bar plot)** ƒë·ªÉ tr·ª±c quan h√≥a **t·ª∑ l·ªá ph·∫ßn trƒÉm kh√°ch h√†ng trong t·ª´ng c·ª•m**.  
M·ª•c ƒë√≠ch l√† gi√∫p ƒë√°nh gi√° xem **c√°c c·ª•m c√≥ ƒë∆∞·ª£c ph√¢n b·ªë c√¢n b·∫±ng** v√† **c√≥ √Ω nghƒ©a** hay kh√¥ng.

---

‚úÖ Bi·ªÉu ƒë·ªì c·ªôt s·∫Ω gi√∫p:
- Ph√°t hi·ªán xem c√≥ c·ª•m n√†o chi·∫øm **qu√° nhi·ªÅu** ho·∫∑c **qu√° √≠t** kh√°ch h√†ng kh√¥ng.
- H·ªó tr·ª£ vi·ªác **ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m** tr∆∞·ªõc khi ƒëi s√¢u v√†o ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm t·ª´ng c·ª•m.

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# T√≠nh ph·∫ßn trƒÉm kh√°ch h√†ng trong t·ª´ng c·ª•m
cluster_percentage = (customer_data_cleaned['cluster'].value_counts(normalize=True) * 100).reset_index()
cluster_percentage.columns = ['Cluster', 'Percentage']
cluster_percentage.sort_values(by='Cluster', inplace=True)

# T·∫°o m√†u cho t·ª´ng c·ª•m (n·∫øu b·∫°n mu·ªën custom)
colors = sns.color_palette("husl", len(cluster_percentage))  # Ho·∫∑c b·∫°n ƒë·ªÉ nguy√™n `palette=colors`

# T·∫°o bi·ªÉu ƒë·ªì c·ªôt ngang
plt.figure(figsize=(10, 4))
sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)

# Th√™m nh√£n ph·∫ßn trƒÉm tr√™n t·ª´ng thanh
for index, value in enumerate(cluster_percentage['Percentage']):
    plt.text(value + 0.5, index, f'{value:.2f}%', va='center')

# ƒê·∫∑t ti√™u ƒë·ªÅ v√† tr·ª•c
plt.title('Ph√¢n b·ªë kh√°ch h√†ng theo c√°c c·ª•m', fontsize=14)
plt.xticks(ticks=np.arange(0, 60, 5))
plt.xlabel('T·ª∑ l·ªá (%)')

# Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì
plt.show()

## ƒê√°nh Gi√° Ph√¢n B·ªë Kh√°ch H√†ng Gi·ªØa C√°c C·ª•m

S·ª± ph√¢n b·ªë kh√°ch h√†ng gi·ªØa c√°c c·ª•m, nh∆∞ ƒë∆∞·ª£c minh h·ªça qua **bi·ªÉu ƒë·ªì c·ªôt**, cho th·∫•y m·ªôt ph√¢n b·ªë **h∆°i l·ªách nh·∫π**:

---

### K·∫øt qu·∫£ ph√¢n b·ªë:
- **C·ª•m 2**: ~41.31% t·ªïng s·ªë kh√°ch h√†ng.
- **C·ª•m 0**: ~40.89% t·ªïng s·ªë kh√°ch h√†ng.
- **C·ª•m 1**: ~17.80% t·ªïng s·ªë kh√°ch h√†ng (nh√≥m nh·ªè nh·∫•t).

---

### Nh·∫≠n ƒë·ªãnh:
- Ph√¢n b·ªë n√†y cho th·∫•y thu·∫≠t to√°n ph√¢n c·ª•m ƒë√£ ho·∫°t ƒë·ªông **hi·ªáu qu·∫£** trong vi·ªác **t√°ch bi·ªát c√°c nh√≥m kh√°ch h√†ng** th√†nh c√°c ph√¢n kh√∫c ri√™ng bi·ªát.
- C√≥ **hai ph√¢n kh√∫c l·ªõn** v√† **m·ªôt ph√¢n kh√∫c nh·ªè h∆°n nh∆∞ng v·∫´n c√≥ √Ω nghƒ©a quan tr·ªçng**.
- Nh√≥m kh√°ch h√†ng trong c·ª•m nh·ªè n√†y c√≥ th·ªÉ l√† **nh·ªØng kh√°ch h√†ng ƒë·∫∑c bi·ªát**, c·∫ßn c√≥ c√°c **chi·∫øn l∆∞·ª£c chƒÉm s√≥c ri√™ng bi·ªát**, v√≠ d·ª•:
  - **∆Øu ƒë√£i c√° nh√¢n h√≥a**.
  - **Ch∆∞∆°ng tr√¨nh gi·ªØ ch√¢n kh√°ch h√†ng**.

---

‚úÖ Nh√¨n chung, s·ª± ph√¢n b·ªë n√†y ƒë·∫£m b·∫£o:
- **ƒê·ªô ƒëa d·∫°ng** v√† **ƒë·∫°i di·ªán h·ª£p l√Ω** c·ªßa c√°c ph√¢n kh√∫c kh√°ch h√†ng.
- T·∫°o **n·ªÅn t·∫£ng v·ªØng ch·∫Øc** ƒë·ªÉ doanh nghi·ªáp √°p d·ª•ng c√°c chi·∫øn l∆∞·ª£c d·ª±a tr√™n **d·ªØ li·ªáu th·ª±c ti·ªÖn**.

## Step 11 | Cluster Analysis and Profiling
## Ph√¢n T√≠ch ƒê·∫∑c ƒêi·ªÉm v√† H·ªì S∆° Kh√°ch H√†ng Theo T·ª´ng C·ª•m

Trong ph·∫ßn n√†y, t√¥i s·∫Ω th·ª±c hi·ªán **ph√¢n t√≠ch c√°c ƒë·∫∑c ƒëi·ªÉm c·ªßa t·ª´ng c·ª•m** ƒë·ªÉ hi·ªÉu r√µ:
- **H√†nh vi**.
- **S·ªü th√≠ch kh√°c bi·ªát** c·ªßa c√°c nh√≥m kh√°ch h√†ng.

---

### M·ª•c ti√™u:
‚úÖ Hi·ªÉu ƒë∆∞·ª£c **s·ª± kh√°c bi·ªát n·ªïi b·∫≠t** gi·ªØa c√°c c·ª•m kh√°ch h√†ng.  
‚úÖ X√¢y d·ª±ng **h·ªì s∆° cho t·ª´ng c·ª•m** nh·∫±m x√°c ƒë·ªãnh:
- **C√°c ƒë·∫∑c ƒëi·ªÉm ch√≠nh n·ªïi b·∫≠t**.
- Nh·ªØng **h√†nh vi ƒë·∫∑c tr∆∞ng** c·ªßa kh√°ch h√†ng trong m·ªói nh√≥m.

---

üéØ K·∫øt qu·∫£ c·ªßa ph√¢n t√≠ch n√†y s·∫Ω gi√∫p:
- Doanh nghi·ªáp **n·∫Øm b·∫Øt ch√¢n dung kh√°ch h√†ng r√µ r√†ng h∆°n**.
- T·ª´ ƒë√≥ ƒë∆∞a ra c√°c **chi·∫øn l∆∞·ª£c marketing**, **∆∞u ƒë√£i c√° nh√¢n h√≥a**, ho·∫∑c **ch∆∞∆°ng tr√¨nh gi·ªØ ch√¢n kh√°ch h√†ng** ph√π h·ª£p cho t·ª´ng ph√¢n kh√∫c.

# Step 11.1 | Radar Chart Approach
## Tr·ª±c Quan H√≥a H·ªì S∆° C√°c C·ª•m B·∫±ng Bi·ªÉu ƒê·ªì Radar

Tr∆∞·ªõc ti√™n, t√¥i s·∫Ω **t·∫°o c√°c bi·ªÉu ƒë·ªì radar** ƒë·ªÉ tr·ª±c quan h√≥a **gi√° tr·ªã t√¢m (centroid)** c·ªßa t·ª´ng c·ª•m tr√™n c√°c ƒë·∫∑c tr∆∞ng kh√°c nhau.  
Vi·ªác n√†y gi√∫p **so s√°nh nhanh ch√≥ng** h·ªì s∆° c·ªßa c√°c c·ª•m kh√°ch h√†ng kh√°c nhau.

---

### C√°c b∆∞·ªõc th·ª±c hi·ªán:

1Ô∏è‚É£ **T√≠nh to√°n gi√° tr·ªã t√¢m (centroid)** cho t·ª´ng c·ª•m:
- T√¢m c·ª•m ƒë·∫°i di·ªán cho **gi√° tr·ªã trung b√¨nh** c·ªßa t·∫•t c·∫£ c√°c ƒë·∫∑c tr∆∞ng trong m·ªôt c·ª•m c·ª• th·ªÉ.

2Ô∏è‚É£ **Hi·ªÉn th·ªã c√°c gi√° tr·ªã t√¢m n√†y** tr√™n bi·ªÉu ƒë·ªì radar:
- Bi·ªÉu ƒë·ªì radar gi√∫p **d·ªÖ d√†ng quan s√°t xu h∆∞·ªõng trung t√¢m** c·ªßa t·ª´ng ƒë·∫∑c tr∆∞ng tr√™n c√°c c·ª•m kh√°ch h√†ng kh√°c nhau.
- T·ª´ ƒë√≥, ta c√≥ th·ªÉ nh·∫≠n bi·∫øt:
  - C·ª•m n√†o c√≥ gi√° tr·ªã n·ªïi b·∫≠t ·ªü ƒë·∫∑c tr∆∞ng n√†o.
  - C√°c c·ª•m kh√°c nhau v·ªÅ ƒë·∫∑c ƒëi·ªÉm g√¨.

---

‚úÖ Bi·ªÉu ƒë·ªì radar l√† c√¥ng c·ª• h·ªØu √≠ch ƒë·ªÉ:
- **T√≥m t·∫Øt tr·ª±c quan h·ªì s∆° c·ª•m**.
- H·ªó tr·ª£ ƒë∆∞a ra quy·∫øt ƒë·ªãnh v√† x√¢y d·ª±ng **chi·∫øn l∆∞·ª£c marketing ph√π h·ª£p** cho t·ª´ng nh√≥m kh√°ch h√†ng.

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# 1Ô∏è‚É£ ƒê·∫∑t 'CustomerID' l√†m index v√† chu·∫©n h√≥a d·ªØ li·ªáu (tr·ª´ c·ªôt 'cluster')
df_customer = customer_data_cleaned.set_index('CustomerID')
features = df_customer.drop(columns=['cluster'])

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# T·∫°o DataFrame chu·∫©n h√≥a v√† th√™m 'cluster' tr·ªü l·∫°i
df_customer_standardized = pd.DataFrame(features_scaled, columns=features.columns, index=features.index)
df_customer_standardized['cluster'] = df_customer['cluster']

# 2Ô∏è‚É£ T√≠nh centroid (trung b√¨nh) cho t·ª´ng c·ª•m
cluster_centroids = df_customer_standardized.groupby('cluster').mean()

# 3Ô∏è‚É£ Chu·∫©n b·ªã v·∫Ω Radar Chart
labels = cluster_centroids.columns.tolist()
num_vars = len(labels)

angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
labels += [labels[0]]
angles += angles[:1]

# T·∫°o m√†u cho t·ª´ng c·ª•m
colors = ['#ff6200', '#2ca02c', '#1f77b4']  # B·∫°n c√≥ th·ªÉ ch·ªânh m√†u t·∫°i ƒë√¢y n·∫øu mu·ªën

# H√†m v·∫Ω Radar Chart cho t·ª´ng c·ª•m
def create_radar_chart(ax, angles, data, color, cluster_label):
    ax.fill(angles, data, color=color, alpha=0.3)
    ax.plot(angles, data, color=color, linewidth=2, linestyle='solid')
    ax.set_title(f'C·ª•m {cluster_label}', size=15, color=color, y=1.1)

# 4Ô∏è‚É£ T·∫°o bi·ªÉu ƒë·ªì radar cho t·ª´ng c·ª•m
fig, axs = plt.subplots(figsize=(20, 8), subplot_kw=dict(polar=True), nrows=1, ncols=3)

for i, ax in enumerate(axs):
    cluster_data = cluster_centroids.loc[i].tolist()
    cluster_data += cluster_data[:1]  # ƒê√≥ng v√≤ng
    create_radar_chart(ax, angles, cluster_data, colors[i], i)
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels[:-1], fontsize=10)
    ax.grid(color='grey', linewidth=0.5)

plt.tight_layout()
plt.show()

## Customer Profiles Derived from Radar Chart Analysis

---

### üå∂Ô∏è C·ª•m 0 (Bi·ªÉu ƒë·ªì Cam)
üéØ **H·ªì s∆°:** Kh√°ch h√†ng mua √≠t, giao d·ªãch th·∫•p, chi ti√™u th·∫•p

- `Total_Transactions`, `Total_Products_Purchased`, `Total_Spend` ƒë·ªÅu **r·∫•t th·∫•p** ‚Üí Kh√°ch h√†ng **√≠t mua**, **√≠t chi ti√™u**.
- `Day_Of_Week` **r·∫•t cao** ‚Üí Ch·ªß y·∫øu mua **v√†o cu·ªëi tu·∫ßn**.
- `Spending_Trend` **th·∫•p** ‚Üí Chi ti√™u c√≥ xu h∆∞·ªõng **gi·∫£m d·∫ßn**.
- C√°c ch·ªâ s·ªë kh√°c nh∆∞ `Average_Transaction_Value`, `Cancellation`, `Is_UK` ƒë·ªÅu **th·∫•p**.
- üëâ ƒê√¢y l√† nh√≥m **kh√°ch h√†ng √≠t ho·∫°t ƒë·ªông**, **giao d·ªãch nh·ªè l·∫ª**, **kh√¥ng th∆∞·ªùng xuy√™n**.

---

### üåø C·ª•m 1 (Bi·ªÉu ƒë·ªì Xanh L√°)
üéØ **H·ªì s∆°:** Kh√°ch h√†ng trung th√†nh, mua th∆∞·ªùng xuy√™n, chi ti√™u cao, ƒëa d·∫°ng s·∫£n ph·∫©m

- `Total_Transactions`, `Total_Products_Purchased`, `Total_Spend`, `Unique_Products_Purchased` ƒë·ªÅu **cao v∆∞·ª£t tr·ªôi** ‚Üí Nh√≥m **ho·∫°t ƒë·ªông s√¥i n·ªïi nh·∫•t**.
- `Average_Transaction_Value` **cao v·ª´a ph·∫£i** ‚Üí Giao d·ªãch **gi√° tr·ªã trung b√¨nh**, **th∆∞·ªùng xuy√™n**.
- `Spending_Trend` **cao** ‚Üí Chi ti√™u **tƒÉng tr∆∞·ªüng d∆∞∆°ng**, **b·ªÅn v·ªØng**.
- `Day_Of_Week` v√† `Hour` **th·∫•p** ‚Üí Th∆∞·ªùng mua **ƒë·∫ßu tu·∫ßn** v√† **ban ng√†y**.
- `Is_UK` **cao** ‚Üí Ch·ªß y·∫øu l√† **kh√°ch h√†ng ·ªü UK**.
- `Cancellation Rate` v√† `Frequency` **th·∫•p** ‚Üí **√çt h·ªßy ƒë∆°n h√†ng**.
- üëâ ƒê√¢y l√† **nh√≥m kh√°ch h√†ng trung th√†nh**, ƒë√≥ng g√≥p **doanh thu ch√≠nh**.

---

### üåä C·ª•m 2 (Bi·ªÉu ƒë·ªì Xanh D∆∞∆°ng)
üéØ **H·ªì s∆°:** Kh√°ch h√†ng mua th∆∞a th·ªõt, nh∆∞ng gi√° tr·ªã giao d·ªãch cao

- `Days_Since_Last_Purchase` v√† `Average_Days_Between_Purchases` **cao** ‚Üí **Mua kh√¥ng th∆∞·ªùng xuy√™n**.
- `Average_Transaction_Value`, `Total_Spend` v√† `Spending_Trend` **cao** ‚Üí **M·ªói l·∫ßn mua gi√° tr·ªã l·ªõn**.
- `Is_UK` **cao** ‚Üí Ch·ªß y·∫øu l√† **ng∆∞·ªùi Anh**.
- `Hour` **cao** ‚Üí Th∆∞·ªùng mua **v√†o cu·ªëi ng√†y**.
- `Total_Transactions` v√† `Total_Products_Purchased` **th·∫•p** ‚Üí **√çt mua nh∆∞ng khi mua th√¨ mua l·ªõn**.
- `Monthly_Spending_Std` **v·ª´a ph·∫£i** ‚Üí Chi ti√™u **kh√° ·ªïn ƒë·ªãnh**.
- üëâ ƒê√¢y l√† nh√≥m **VIP th·∫ßm l·∫∑ng** ‚Äì **√≠t mua nh∆∞ng gi√° tr·ªã cao**.

---

### ‚úÖ T√≥m l·∫°i:

| C·ª•m      | H·ªì s∆° Kh√°ch H√†ng                                                                                      |
|----------|--------------------------------------------------------------------------------------------------------|
| üå∂Ô∏è C·ª•m 0 | **Kh√°ch √≠t ho·∫°t ƒë·ªông**, **kh√¥ng th∆∞·ªùng xuy√™n**, **chi ti√™u th·∫•p**                                      |
| üåø C·ª•m 1 | **Kh√°ch trung th√†nh**, **mua th∆∞·ªùng xuy√™n**, **ƒë√≥ng g√≥p doanh thu ch√≠nh**                               |
| üåä C·ª•m 2 | **Kh√°ch √≠t mua nh∆∞ng giao d·ªãch l·ªõn** ‚Äì **t·∫≠p trung v√†o gi√° tr·ªã cao**                                    |

## Step 11.2 | Histogram Chart Approach
## X√°c Th·ª±c H·ªì S∆° Kh√°ch H√†ng B·∫±ng Bi·ªÉu ƒê·ªì Histogram

ƒê·ªÉ x√°c th·ª±c c√°c **h·ªì s∆° kh√°ch h√†ng** ƒë√£ ƒë∆∞·ª£c x√°c ƒë·ªãnh t·ª´ **bi·ªÉu ƒë·ªì radar**, ch√∫ng ta c√≥ th·ªÉ v·∫Ω **c√°c bi·ªÉu ƒë·ªì histogram** cho t·ª´ng **ƒë·∫∑c tr∆∞ng (feature)**, ƒë∆∞·ª£c ph√¢n chia theo **nh√£n c·ª•m (cluster labels)**.

---

### üéØ M·ª•c ƒë√≠ch:
‚úÖ Quan s√°t tr·ª±c quan **s·ª± ph√¢n b·ªë gi√° tr·ªã c·ªßa c√°c ƒë·∫∑c tr∆∞ng trong t·ª´ng c·ª•m**.  
‚úÖ X√°c nh·∫≠n ho·∫∑c **ƒëi·ªÅu ch·ªânh l·∫°i h·ªì s∆° kh√°ch h√†ng** ƒë√£ x√¢y d·ª±ng d·ª±a tr√™n bi·ªÉu ƒë·ªì radar.

---

### üìä Bi·ªÉu ƒë·ªì Histogram gi√∫p:
- So s√°nh **s·ª± kh√°c bi·ªát r√µ r·ªát** gi·ªØa c√°c c·ª•m.
- Hi·ªÉu r√µ h∆°n v·ªÅ **m·ª©c ƒë·ªô lan t·ªèa** v√† **t·∫ßn su·∫•t xu·∫•t hi·ªán** c·ªßa c√°c gi√° tr·ªã trong t·ª´ng c·ª•m.
- Ki·ªÉm tra xem **h·ªì s∆° c·ª•m ƒë√£ x√°c ƒë·ªãnh** c√≥ ph√π h·ª£p v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø hay kh√¥ng.

---

‚úÖ B∆∞·ªõc n√†y l√† m·ªôt ph·∫ßn quan tr·ªçng ƒë·ªÉ **ƒë·∫£m b·∫£o t√≠nh ch√≠nh x√°c** v√† **ƒë·ªô tin c·∫≠y** c·ªßa ph√¢n t√≠ch c·ª•m, tr∆∞·ªõc khi ti·∫øn t·ªõi c√°c b∆∞·ªõc tri·ªÉn khai chi·∫øn l∆∞·ª£c kinh doanh ho·∫∑c g·ª£i √Ω c√° nh√¢n h√≥a.

import matplotlib.pyplot as plt
import numpy as np

# L·∫•y danh s√°ch c√°c ƒë·∫∑c tr∆∞ng (b·ªè c·ªôt CustomerID v√† cluster)
features = customer_data_cleaned.columns.difference(['CustomerID', 'cluster']).tolist()
clusters = sorted(customer_data_cleaned['cluster'].unique())  # L·∫•y danh s√°ch c·ª•m v√† s·∫Øp x·∫øp

# M√†u cho c√°c c·ª•m (t√πy ch·ªânh theo s·ªë c·ª•m)
colors = ['#ff6200', '#2ca02c', '#1f77b4']

# Thi·∫øt l·∫≠p h·ªá th·ªëng subplot
n_rows = len(features)
n_cols = len(clusters)
fig, axes = plt.subplots(n_rows, n_cols, figsize=(4*n_cols, 2.5*n_rows), squeeze=False)

# V·∫Ω histogram cho t·ª´ng feature v√† t·ª´ng cluster
for i, feature in enumerate(features):
    for j, cluster in enumerate(clusters):
        data = customer_data_cleaned[customer_data_cleaned['cluster'] == cluster][feature]
        ax = axes[i, j]
        ax.hist(data, bins=20, color=colors[j % len(colors)], edgecolor='w', alpha=0.7)
        ax.set_title(f'Cluster {cluster} - {feature}', fontsize=12)
        ax.set_xlabel('')
        ax.set_ylabel('')
        ax.grid(axis='y', linestyle='--', linewidth=0.5, alpha=0.5)

# G·ª° nh√£n tr·ª•c ƒë·ªÉ g·ªçn
for ax in axes.flat:
    ax.label_outer()

plt.tight_layout()
plt.show()


The detailed insights from the histograms provide a more nuanced understanding of each cluster, helping in refining the profiles to represent the customer behaviors more accurately. Based on the detailed analysis from both the radar charts and the histograms, here are the refined profiles and titles for each cluster:
# Ti·∫øng Vi·ªát
Nh·ªØng hi·ªÉu bi·∫øt chi ti·∫øt t·ª´ c√°c bi·ªÉu ƒë·ªì histogram cung c·∫•p m·ªôt c√°i nh√¨n s√¢u s·∫Øc h∆°n v·ªÅ t·ª´ng c·ª•m, gi√∫p tinh ch·ªânh c√°c h·ªì s∆° kh√°ch h√†ng ƒë·ªÉ ph·∫£n √°nh ch√≠nh x√°c h∆°n h√†nh vi c·ªßa h·ªç. D·ª±a tr√™n ph√¢n t√≠ch chi ti·∫øt t·ª´ c·∫£ bi·ªÉu ƒë·ªì radar v√† bi·ªÉu ƒë·ªì histogram, d∆∞·ªõi ƒë√¢y l√† c√°c h·ªì s∆° ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh v√† ti√™u ƒë·ªÅ ph√π h·ª£p cho t·ª´ng c·ª•m kh√°ch h√†ng:
## Ph√¢n T√≠ch H·ªì S∆° Kh√°ch H√†ng Theo C·ª•m

---

### üüß Cluster 0 ‚Äì Low-Frequency, Low-Spend Buyers
üéØ **H·ªì s∆°:** Kh√°ch h√†ng mua √≠t, giao d·ªãch nh·ªè, chi ti√™u th·∫•p, mua r·∫£i r√°c

- T·∫ßn su·∫•t giao d·ªãch v√† s·ªë s·∫£n ph·∫©m mua **th·∫•p nh·∫•t** trong 3 c·ª•m.
- **Gi√° tr·ªã giao d·ªãch trung b√¨nh th·∫•p**, t·ªïng chi ti√™u th·∫•p ‚Üí **nh√≥m √≠t mua, mua nh·ªè l·∫ª**.
- Ng√†y mua s·∫Øm **ph√¢n t√°n**, ch·ªß y·∫øu v√†o **gi·ªØa tu·∫ßn**.  
  Gi·ªù mua t·∫≠p trung v√†o **bu·ªïi chi·ªÅu** (11h‚Äì16h).
- `Spending_Trend` g·∫ßn nh∆∞ **kh√¥ng ƒë·ªïi**, h√†nh vi chi ti√™u ·ªïn ƒë·ªãnh nh∆∞ng **th·∫•p**.
- T·ª∑ l·ªá h·ªßy ƒë∆°n **th·∫•p**, kh√¥ng c√≥ d·∫•u hi·ªáu mua b·ªëc ƒë·ªìng.
- Chi ti√™u h√†ng th√°ng **dao ƒë·ªông nh·ªè**, kh√¥ng bi·∫øn ƒë·ªông l·ªõn.
- `Is_UK` ph√¢n b·ªë **trung b√¨nh**, kh√¥ng qu√° t·∫≠p trung v√†o kh√°ch UK.

üëâ **K·∫øt lu·∫≠n:** Nh√≥m kh√°ch h√†ng **nh·ªè l·∫ª**, √≠t mua, chi ti√™u th·∫•p ‚Üí **kh√¥ng ph·∫£i kh√°ch h√†ng ti·ªÅm nƒÉng cao**.

---

### üü© Cluster 1 ‚Äì High-Volume Shoppers with Moderate Spending
üéØ **H·ªì s∆°:** Kh√°ch h√†ng mua nhi·ªÅu, giao d·ªãch d√†y, chi ti√™u kh√° cao

- T·∫ßn su·∫•t giao d·ªãch v√† s·ªë s·∫£n ph·∫©m mua **cao nh·∫•t** ‚Üí **nh√≥m mua nhi·ªÅu nh·∫•t**.
- Gi√° tr·ªã giao d·ªãch trung b√¨nh **kh√¥ng qu√° cao**, nh∆∞ng t·ªïng chi ti√™u **cao** nh·ªù **mua nhi·ªÅu l·∫ßn**.
- Ng√†y mua d·ªìn v√†o **ƒë·∫ßu tu·∫ßn (th·ª© 2‚Äì4)**.  
  Gi·ªù mua t·∫≠p trung v√†o **bu·ªïi s√°ng v√† ƒë·∫ßu gi·ªù chi·ªÅu (8h‚Äì14h)**.
- `Spending_Trend` **tƒÉng d∆∞∆°ng r√µ r·ªát**, chi ti√™u ng√†y c√†ng nhi·ªÅu.
- T·ª∑ l·ªá h·ªßy ƒë∆°n **th·∫•p**, h√†nh vi mua **·ªïn ƒë·ªãnh, c√≥ k·∫ø ho·∫°ch**.
- `Is_UK` **cao**, ch·ªß y·∫øu l√† kh√°ch h√†ng UK.
- `Monthly_Spending_Mean` v√† `Std` **cao**, chi ti√™u nhi·ªÅu v√† bi·∫øn ƒë·ªông theo th·ªùi gian.

üëâ **K·∫øt lu·∫≠n:** ƒê√¢y l√† **kh√°ch h√†ng trung th√†nh, c√≥ gi√° tr·ªã cao** ‚Üí **∆∞u ti√™n gi·ªØ ch√¢n**, tƒÉng c∆∞·ªùng **upsell/cross-sell**.

---

### üü¶ Cluster 2 ‚Äì Big Spenders with Occasional Shopping
üéØ **H·ªì s∆°:** Kh√°ch h√†ng chi ti√™u cao nh∆∞ng √≠t mua, c√≥ h√†nh vi b·ªëc ƒë·ªìng

- T·∫ßn su·∫•t giao d·ªãch v√† s·ªë s·∫£n ph·∫©m mua **trung b√¨nh**.
- Gi√° tr·ªã giao d·ªãch trung b√¨nh **r·∫•t cao** ‚Üí **√≠t mua nh∆∞ng chi ƒë·∫≠m m·ªói l·∫ßn**.
- Ng√†y mua t·∫≠p trung v√†o **cu·ªëi tu·∫ßn (th·ª© 5‚Äì7)**.  
  Gi·ªù mua th∆∞·ªùng l√† **cu·ªëi bu·ªïi chi·ªÅu v√† t·ªëi (16h‚Äì21h)**.
- `Spending_Trend` **gi·∫£m nh·∫π**, chi ti√™u ƒëang c√≥ xu h∆∞·ªõng gi·∫£m.
- T·ª∑ l·ªá h·ªßy ƒë∆°n **cao nh·∫•t** trong 3 c·ª•m ‚Üí kh·∫£ nƒÉng **mua b·ªëc ƒë·ªìng ho·∫∑c ƒë·ªïi √Ω sau mua**.
- Chi ti√™u h√†ng th√°ng **bi·∫øn ƒë·ªông m·∫°nh**, th·ªÉ hi·ªán s·ª± kh√¥ng ·ªïn ƒë·ªãnh.
- `Is_UK` **cao**, nhi·ªÅu kh√°ch h√†ng ƒë·∫øn t·ª´ UK.

üëâ **K·∫øt lu·∫≠n:** Nh√≥m **kh√°ch h√†ng ti·ªÅm nƒÉng nh∆∞ng r·ªßi ro cao** ‚Üí c·∫ßn **gi·ªØ ch√¢n**, gi·∫£m **t·ª∑ l·ªá h·ªßy ƒë∆°n** v√† t·ªëi ∆∞u **chi ti√™u d√†i h·∫°n**.

---

‚úÖ Ph√¢n t√≠ch n√†y l√† c∆° s·ªü v·ªØng ch·∫Øc ƒë·ªÉ x√¢y d·ª±ng c√°c **chi·∫øn l∆∞·ª£c chƒÉm s√≥c kh√°ch h√†ng**, **∆∞u ƒë√£i c√° nh√¢n h√≥a**, v√† **t·ªëi ∆∞u h√≥a doanh thu** cho t·ª´ng nh√≥m.
